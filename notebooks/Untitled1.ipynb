{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41457af3-580e-4a2f-93bd-db2801e222b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://25980bb9571e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data Quality</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x75be46775b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data Quality\").getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd40ef85-0ad5-42d9-8da0-5bf18007e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"FakeTrainingData\").getOrCreate()\n",
    "\n",
    "# Generate fake data\n",
    "data = [\n",
    "    (44, 9, 21.0),\n",
    "    (10, 5, 53.0),\n",
    "    (37, 6, 80.0),\n",
    "    (19, 10, 11.0),\n",
    "    (46, 13, 215.0)\n",
    "]\n",
    "\n",
    "# Add more rows to reach 543 samples\n",
    "for _ in range(543 - len(data)):\n",
    "    totalAdClicks = random.randint(1, 60)\n",
    "    totalBuyClicks = random.randint(1, 15)\n",
    "    totalRevenue = round(random.uniform(5, 250), 1)\n",
    "    data.append((totalAdClicks, totalBuyClicks, totalRevenue))\n",
    "\n",
    "# Define schema\n",
    "columns = [\"SumAddClicks\", \"SumBuyClicks\", \"Revenue\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fd1538-d214-45a4-82aa-b581b58e79cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------+\n",
      "|SumAddClicks|SumBuyClicks|Revenue|\n",
      "+------------+------------+-------+\n",
      "|          44|           9|   21.0|\n",
      "|          10|           5|   53.0|\n",
      "|          37|           6|   80.0|\n",
      "|          19|          10|   11.0|\n",
      "|          46|          13|  215.0|\n",
      "+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "(543, 3)\n"
     ]
    }
   ],
   "source": [
    "trainingDF = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show first 5 rows\n",
    "trainingDF.show(5)\n",
    "\n",
    "# Show dimensions (rows, columns)\n",
    "rows = trainingDF.count()\n",
    "cols = len(trainingDF.columns)\n",
    "print((rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0c8f1a-58f6-4abe-b138-d6dffe990cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "rows.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68dda5f-9e3f-4edd-846d-653429095c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark ready: 3.5.3\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 178e07f0-0f5b-44f8-9bd2-4a191f6bef48, runId = f79eb2d7-b1de-4bb2-aab3-f21030ddbab9] terminated with exception: Set(transactions-6, transactions-4, transactions-1, transactions-7, transactions-5, transactions-2, transactions-3) are gone. Some data may have been missed.. \nSome data may have been lost because they are not available in Kafka any more; either the\n data was aged out by Kafka or the topic may have been deleted before all the data in the\n topic was processed. If you don't want your streaming query to fail on such cases, set the\n source option \"failOnDataLoss\" to \"false\".\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 117\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Write stream\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    111\u001b[0m query \u001b[38;5;241m=\u001b[39m joined\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(\u001b[38;5;28;01mlambda\u001b[39;00m df, bid: (df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), send_metrics(df, bid))) \\\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/output/fraud_checkpoint_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m--> 117\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m query\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/streaming/query.py:219\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    216\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_NOT_POSITIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(timeout)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    218\u001b[0m         )\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 178e07f0-0f5b-44f8-9bd2-4a191f6bef48, runId = f79eb2d7-b1de-4bb2-aab3-f21030ddbab9] terminated with exception: Set(transactions-6, transactions-4, transactions-1, transactions-7, transactions-5, transactions-2, transactions-3) are gone. Some data may have been missed.. \nSome data may have been lost because they are not available in Kafka any more; either the\n data was aged out by Kafka or the topic may have been deleted before all the data in the\n topic was processed. If you don't want your streaming query to fail on such cases, set the\n source option \"failOnDataLoss\" to \"false\".\n    "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr, concat, lit\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, LongType\n",
    "import pyspark.sql.functions as F\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "BOOTSTRAP = \"broker:29094\"   # inside Docker network\n",
    "TXN_TOPIC = \"transaction\"\n",
    "METRICS_TOPIC = \"metrics\"\n",
    "mode = \"salting\"   # \"baseline\", \"broadcast\", or \"salting\"\n",
    "enable_aqe = True\n",
    "SKEW_CARD = \"4111-1111-1111-1111\"\n",
    "# ----------------------------\n",
    "# Spark Session\n",
    "# ----------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"fraud-detection-demo\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"6\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark ready:\", spark.version)\n",
    "\n",
    "if enable_aqe:\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# ----------------------------\n",
    "# Input schema & Kafka source\n",
    "# ----------------------------\n",
    "schema = StructType() \\\n",
    "    .add(\"txn_id\", LongType()) \\\n",
    "    .add(\"card_number\", StringType()) \\\n",
    "    .add(\"amount\", DoubleType()) \\\n",
    "    .add(\"merchant\", StringType()) \\\n",
    "    .add(\"ts\", LongType())\n",
    "\n",
    "kdf = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", BOOTSTRAP) \\\n",
    "    .option(\"subscribe\", TXN_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "txn_df = kdf.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"j\")).select(\"j.*\")\n",
    "\n",
    "# ----------------------------\n",
    "# Lookup table (risk profiles)\n",
    "# ----------------------------\n",
    "risk_profiles = spark.createDataFrame([\n",
    "    (\"4111-1111-1111-1111\", \"high\"),\n",
    "    (\"4000-0000-0000-0002\", \"medium\"),\n",
    "    (\"4000-0000-0000-0003\", \"low\")\n",
    "], [\"card_number\", \"risk_level\"])\n",
    "\n",
    "# ----------------------------\n",
    "# Join strategies\n",
    "# ----------------------------\n",
    "if mode == \"baseline\":\n",
    "    joined = txn_df.join(risk_profiles, \"card_number\", \"left\")\n",
    "\n",
    "elif mode == \"broadcast\":\n",
    "    joined = txn_df.join(F.broadcast(risk_profiles), \"card_number\", \"left\")\n",
    "\n",
    "elif mode == \"salting\":\n",
    "    SALT_N = 6\n",
    "    salts = spark.range(0, SALT_N).selectExpr(\"id as salt\")\n",
    "    lookup_salted = risk_profiles.crossJoin(salts) \\\n",
    "        .withColumn(\"salted_card\", concat(col(\"card_number\"), lit(\"_\"), col(\"salt\"))) \\\n",
    "        .select(\"salted_card\", \"risk_level\")\n",
    "\n",
    "    salted_stream = txn_df.withColumn(\n",
    "        \"salt\",\n",
    "        expr(f\"CASE WHEN card_number='{SKEW_CARD}' THEN floor(rand()*{SALT_N}) ELSE 0 END\")\n",
    "    ).withColumn(\"salted_card\", concat(col(\"card_number\"), lit(\"_\"), col(\"salt\")))\n",
    "\n",
    "    joined = salted_stream.join(\n",
    "        lookup_salted,\n",
    "        salted_stream.salted_card == lookup_salted.salted_card,\n",
    "        \"left\"\n",
    "    ).drop(\"salted_card\")\n",
    "\n",
    "# ----------------------------\n",
    "# Metrics sender\n",
    "# ----------------------------\n",
    "def send_metrics(batch_df, batch_id):\n",
    "    total = batch_df.count()\n",
    "    risky = batch_df.filter(\"risk_level='high'\").count()\n",
    "    metrics = {\n",
    "        \"batch_id\": int(batch_id),\n",
    "        \"mode\": mode,\n",
    "        \"total_txns\": int(total),\n",
    "        \"high_risk_txns\": int(risky),\n",
    "        \"fraud_ratio\": round(risky / total, 3) if total > 0 else 0\n",
    "    }\n",
    "    print(f\"[Metrics] {metrics}\")\n",
    "\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=BOOTSTRAP,\n",
    "        value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    "    )\n",
    "    producer.send(METRICS_TOPIC, value=metrics)\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "\n",
    "# ----------------------------\n",
    "# Write stream\n",
    "# ----------------------------\n",
    "query = joined.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(lambda df, bid: (df.show(5, truncate=False), send_metrics(df, bid))) \\\n",
    "    .option(\"checkpointLocation\", f\"/opt/output/fraud_checkpoint_{mode}\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(60)\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946fde28-3726-40ed-b56c-7e3d8d952680",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/new_checkpoint_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/new_checkpoint_folder\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3e390-0ecc-4573-9f06-db98316252e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
