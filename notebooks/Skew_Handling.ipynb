{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c1c442-a8ab-42aa-96f1-532d3b06c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data Skew Example\").master(\"local[*]\").getOrCreate();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b46f9a-ead4-497e-8adb-671eaa5386b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://042ea7b70ba9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data Skew Example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7e10f4c62c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3443e88-3d40-453b-8eb0-38b450936f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/opt/data/ncr_ride_bookings.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd26d88f-83e3-43b5-9dca-70f6a25d8a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+------------------+---------+---------------------------------+------------------+-----------------+\n",
      "|distinct_values|min_count|median_count|mean_count        |max_count|column                           |skew_ratio        |skew_class       |\n",
      "+---------------+---------+------------+------------------+---------+---------------------------------+------------------+-----------------+\n",
      "|365            |355      |411         |410.958904109589  |462      |Date                             |1.1242            |Balanced         |\n",
      "|62910          |1        |2           |2.384358607534573 |16       |Time                             |6.7104            |Moderately Skewed|\n",
      "|148767         |1        |1           |1.008288128415576 |3        |Booking ID                       |2.97534           |Moderately Skewed|\n",
      "|5              |9000     |10500       |30000.0           |93000    |Booking Status                   |3.1               |Moderately Skewed|\n",
      "|148788         |1        |1           |1.008145818211146 |3        |Customer ID                      |2.97576           |Moderately Skewed|\n",
      "|7              |4449     |22517       |21428.571428571428|37419    |Vehicle Type                     |1.74622           |Balanced         |\n",
      "|176            |790      |849         |852.2727272727273 |949      |Pickup Location                  |1.1134933333333334|Balanced         |\n",
      "|176            |774      |850         |852.2727272727273 |936      |Drop Location                    |1.09824           |Balanced         |\n",
      "|182            |38       |872         |824.1758241758242 |10500    |Avg VTAT                         |12.739999999999998|Highly Skewed    |\n",
      "|352            |23       |325         |426.1363636363636 |48000    |Avg CTAT                         |112.64            |Highly Skewed    |\n",
      "|2              |10500    |10500       |75000.0           |139500   |Cancelled Rides by Customer      |1.86              |Balanced         |\n",
      "|6              |1155     |2335        |25000.0           |139500   |Reason for cancelling by Customer|5.58              |Moderately Skewed|\n",
      "|2              |27000    |27000       |75000.0           |123000   |Cancelled Rides by Driver        |1.64              |Balanced         |\n",
      "|5              |6686     |6751        |30000.0           |123000   |Driver Cancellation Reason       |4.1               |Moderately Skewed|\n",
      "|2              |9000     |9000        |75000.0           |141000   |Incomplete Rides                 |1.88              |Balanced         |\n",
      "|4              |2948     |3012        |37500.0           |141000   |Incomplete Rides Reason          |3.76              |Moderately Skewed|\n",
      "|2567           |1        |10          |58.4339696143358  |48000    |Booking Value                    |821.4399999999999 |Highly Skewed    |\n",
      "|4902           |1        |21          |30.599755201958384|48000    |Ride Distance                    |1568.64           |Highly Skewed    |\n",
      "|22             |745      |3790        |6818.181818181818 |57000    |Driver Ratings                   |8.36              |Moderately Skewed|\n",
      "|22             |443      |2370        |6818.181818181818 |57000    |Customer Rating                  |8.36              |Moderately Skewed|\n",
      "+---------------+---------+------------+------------------+---------+---------------------------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🔎 1. What is Skew?\n",
    "\n",
    "# In distributed systems like Spark, data is partitioned across many worker nodes.\n",
    "# 👉 Skew happens when some keys have way more rows than others.\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def skew_summary(df):\n",
    "    results = []\n",
    "    for col in df.columns:\n",
    "        stats = (\n",
    "            df.groupBy(col).count()\n",
    "              .agg(\n",
    "                  F.count(\"*\").alias(\"distinct_values\"),\n",
    "                  F.min(\"count\").alias(\"min_count\"),\n",
    "                  F.expr(\"percentile_approx(count, 0.5)\").alias(\"median_count\"),\n",
    "                  F.mean(\"count\").alias(\"mean_count\"),\n",
    "                  F.max(\"count\").alias(\"max_count\")\n",
    "              )\n",
    "              .withColumn(\"column\", F.lit(col))\n",
    "              .withColumn(\"skew_ratio\", F.col(\"max_count\") / F.col(\"mean_count\"))\n",
    "              .withColumn(\n",
    "                  \"skew_class\",\n",
    "                  F.when(F.col(\"skew_ratio\") <= 2, \"Balanced\")\n",
    "                   .when((F.col(\"skew_ratio\") > 2) & (F.col(\"skew_ratio\") <= 10), \"Moderately Skewed\")\n",
    "                   .otherwise(\"Highly Skewed\")\n",
    "              )\n",
    "        )\n",
    "        results.append(stats)\n",
    "    \n",
    "    # Merge all per-column results into one DataFrame\n",
    "    summary_df = reduce(lambda a, b: a.unionByName(b), results)\n",
    "    return summary_df\n",
    "\n",
    "# Run summary\n",
    "summary_df = skew_summary(df)\n",
    "summary_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a58b911-92cb-4ee7-aab6-99d0ce78d5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2024, 3, 23), Time=datetime.datetime(2025, 9, 14, 12, 29, 38), Booking ID='\"\"\"CNR5884300\"\"\"', Booking Status='No Driver Found', Customer ID='\"\"\"CID1982111\"\"\"', Vehicle Type='eBike', Pickup Location='Palam Vihar', Drop Location='Jhilmil', Avg VTAT='null', Avg CTAT='null', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='null', Ride Distance='null', Driver Ratings='null', Customer Rating='null', Payment Method='null'),\n",
       " Row(Date=datetime.date(2024, 11, 29), Time=datetime.datetime(2025, 9, 14, 18, 1, 39), Booking ID='\"\"\"CNR1326809\"\"\"', Booking Status='Incomplete', Customer ID='\"\"\"CID4604802\"\"\"', Vehicle Type='Go Sedan', Pickup Location='Shastri Nagar', Drop Location='Gurgaon Sector 56', Avg VTAT='4.9', Avg CTAT='14.0', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='1', Incomplete Rides Reason='Vehicle Breakdown', Booking Value='237', Ride Distance='5.73', Driver Ratings='null', Customer Rating='null', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 8, 23), Time=datetime.datetime(2025, 9, 14, 8, 56, 10), Booking ID='\"\"\"CNR8494506\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID9202816\"\"\"', Vehicle Type='Auto', Pickup Location='Khandsa', Drop Location='Malviya Nagar', Avg VTAT='13.4', Avg CTAT='25.8', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='627', Ride Distance='13.58', Driver Ratings='4.9', Customer Rating='4.9', Payment Method='Debit Card'),\n",
       " Row(Date=datetime.date(2024, 10, 21), Time=datetime.datetime(2025, 9, 14, 17, 17, 25), Booking ID='\"\"\"CNR8906825\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID2610914\"\"\"', Vehicle Type='Premier Sedan', Pickup Location='Central Secretariat', Drop Location='Inderlok', Avg VTAT='13.1', Avg CTAT='28.5', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='416', Ride Distance='34.02', Driver Ratings='4.6', Customer Rating='5.0', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 9, 16), Time=datetime.datetime(2025, 9, 14, 22, 8), Booking ID='\"\"\"CNR1950162\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID9933542\"\"\"', Vehicle Type='Bike', Pickup Location='Ghitorni Village', Drop Location='Khan Market', Avg VTAT='5.3', Avg CTAT='19.6', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='737', Ride Distance='48.21', Driver Ratings='4.1', Customer Rating='4.3', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 2, 6), Time=datetime.datetime(2025, 9, 14, 9, 44, 56), Booking ID='\"\"\"CNR4096693\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID4670564\"\"\"', Vehicle Type='Auto', Pickup Location='AIIMS', Drop Location='Narsinghpur', Avg VTAT='5.1', Avg CTAT='18.1', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='316', Ride Distance='4.85', Driver Ratings='4.1', Customer Rating='4.6', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 6, 17), Time=datetime.datetime(2025, 9, 14, 15, 45, 58), Booking ID='\"\"\"CNR2002539\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID6800553\"\"\"', Vehicle Type='Go Mini', Pickup Location='Vaishali', Drop Location='Punjabi Bagh', Avg VTAT='7.1', Avg CTAT='20.4', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='640', Ride Distance='41.24', Driver Ratings='4.0', Customer Rating='4.1', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 3, 19), Time=datetime.datetime(2025, 9, 14, 17, 37, 37), Booking ID='\"\"\"CNR6568000\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID8610436\"\"\"', Vehicle Type='Auto', Pickup Location='Mayur Vihar', Drop Location='Cyber Hub', Avg VTAT='12.1', Avg CTAT='16.5', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='136', Ride Distance='6.56', Driver Ratings='4.4', Customer Rating='4.2', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 9, 14), Time=datetime.datetime(2025, 9, 14, 12, 49, 9), Booking ID='\"\"\"CNR4510807\"\"\"', Booking Status='No Driver Found', Customer ID='\"\"\"CID7873618\"\"\"', Vehicle Type='Go Sedan', Pickup Location='Noida Sector 62', Drop Location='Noida Sector 18', Avg VTAT='null', Avg CTAT='null', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='null', Ride Distance='null', Driver Ratings='null', Customer Rating='null', Payment Method='null'),\n",
       " Row(Date=datetime.date(2024, 12, 16), Time=datetime.datetime(2025, 9, 14, 19, 6, 48), Booking ID='\"\"\"CNR7721892\"\"\"', Booking Status='Incomplete', Customer ID='\"\"\"CID5214275\"\"\"', Vehicle Type='Auto', Pickup Location='Rohini', Drop Location='Adarsh Nagar', Avg VTAT='6.1', Avg CTAT='26.0', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='1', Incomplete Rides Reason='Other Issue', Booking Value='135', Ride Distance='10.36', Driver Ratings='null', Customer Rating='null', Payment Method='Cash'),\n",
       " Row(Date=datetime.date(2024, 6, 14), Time=datetime.datetime(2025, 9, 14, 16, 24, 12), Booking ID='\"\"\"CNR9070334\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID6680340\"\"\"', Vehicle Type='Auto', Pickup Location='Udyog Bhawan', Drop Location='Dwarka Sector 21', Avg VTAT='7.7', Avg CTAT='18.9', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='181', Ride Distance='19.84', Driver Ratings='4.2', Customer Rating='4.9', Payment Method='Cash'),\n",
       " Row(Date=datetime.date(2024, 9, 18), Time=datetime.datetime(2025, 9, 14, 8, 9, 38), Booking ID='\"\"\"CNR9551927\"\"\"', Booking Status='No Driver Found', Customer ID='\"\"\"CID7568143\"\"\"', Vehicle Type='Auto', Pickup Location='Vidhan Sabha', Drop Location='AIIMS', Avg VTAT='null', Avg CTAT='null', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='null', Ride Distance='null', Driver Ratings='null', Customer Rating='null', Payment Method='null'),\n",
       " Row(Date=datetime.date(2024, 6, 25), Time=datetime.datetime(2025, 9, 14, 22, 44, 15), Booking ID='\"\"\"CNR4386945\"\"\"', Booking Status='Cancelled by Driver', Customer ID='\"\"\"CID5543520\"\"\"', Vehicle Type='eBike', Pickup Location='Patel Chowk', Drop Location='Kherki Daula Toll', Avg VTAT='4.6', Avg CTAT='null', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='1', Driver Cancellation Reason='Personal & Car related issues', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='null', Ride Distance='null', Driver Ratings='null', Customer Rating='null', Payment Method='null'),\n",
       " Row(Date=datetime.date(2024, 9, 11), Time=datetime.datetime(2025, 9, 14, 19, 29, 39), Booking ID='\"\"\"CNR2987763\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID2669710\"\"\"', Vehicle Type='Go Mini', Pickup Location='Malviya Nagar', Drop Location='Ghitorni Village', Avg VTAT='12.2', Avg CTAT='28.2', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='394', Ride Distance='21.44', Driver Ratings='4.1', Customer Rating='4.7', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 10, 18), Time=datetime.datetime(2025, 9, 14, 18, 28, 53), Booking ID='\"\"\"CNR8962232\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID1789354\"\"\"', Vehicle Type='Go Mini', Pickup Location='Madipur', Drop Location='GTB Nagar', Avg VTAT='14.0', Avg CTAT='30.9', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='836', Ride Distance='39.55', Driver Ratings='4.7', Customer Rating='4.4', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 6, 7), Time=datetime.datetime(2025, 9, 14, 15, 5, 35), Booking ID='\"\"\"CNR2390352\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID5432215\"\"\"', Vehicle Type='Auto', Pickup Location='Jama Masjid', Drop Location='Khan Market', Avg VTAT='8.5', Avg CTAT='36.9', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='410', Ride Distance='34.76', Driver Ratings='4.0', Customer Rating='4.9', Payment Method='Uber Wallet'),\n",
       " Row(Date=datetime.date(2024, 7, 1), Time=datetime.datetime(2025, 9, 14, 10, 51, 16), Booking ID='\"\"\"CNR3221338\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID2581698\"\"\"', Vehicle Type='Premier Sedan', Pickup Location='IGI Airport', Drop Location='Madipur', Avg VTAT='5.6', Avg CTAT='27.5', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='401', Ride Distance='21.97', Driver Ratings='4.9', Customer Rating='4.3', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 12, 15), Time=datetime.datetime(2025, 9, 14, 15, 8, 25), Booking ID='\"\"\"CNR6739317\"\"\"', Booking Status='Cancelled by Driver', Customer ID='\"\"\"CID8682675\"\"\"', Vehicle Type='Go Sedan', Pickup Location='Vinobapuri', Drop Location='GTB Nagar', Avg VTAT='6.0', Avg CTAT='null', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='1', Driver Cancellation Reason='Customer related issue', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='null', Ride Distance='null', Driver Ratings='null', Customer Rating='null', Payment Method='null'),\n",
       " Row(Date=datetime.date(2024, 11, 24), Time=datetime.datetime(2025, 9, 14, 9, 7, 10), Booking ID='\"\"\"CNR6126048\"\"\"', Booking Status='Cancelled by Customer', Customer ID='\"\"\"CID1060329\"\"\"', Vehicle Type='eBike', Pickup Location='Kashmere Gate', Drop Location='Anand Vihar', Avg VTAT='12.4', Avg CTAT='null', Cancelled Rides by Customer='1', Reason for cancelling by Customer='Driver is not moving towards pickup location', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='null', Ride Distance='null', Driver Ratings='null', Customer Rating='null', Payment Method='null'),\n",
       " Row(Date=datetime.date(2024, 5, 24), Time=datetime.datetime(2025, 9, 14, 19, 53, 57), Booking ID='\"\"\"CNR9465840\"\"\"', Booking Status='Cancelled by Driver', Customer ID='\"\"\"CID9046501\"\"\"', Vehicle Type='eBike', Pickup Location='Pitampura', Drop Location='Rajiv Nagar', Avg VTAT='10.3', Avg CTAT='null', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='1', Driver Cancellation Reason='Customer related issue', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='null', Ride Distance='null', Driver Ratings='null', Customer Rating='null', Payment Method='null')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75cc506-878f-4e56-b40d-d5a1b6f9d460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE SALTING ===\n",
      "+---------------------+-----+\n",
      "|Booking Status       |count|\n",
      "+---------------------+-----+\n",
      "|Completed            |93000|\n",
      "|Cancelled by Driver  |27000|\n",
      "|No Driver Found      |10500|\n",
      "|Cancelled by Customer|10500|\n",
      "|Incomplete           |9000 |\n",
      "+---------------------+-----+\n",
      "\n",
      "=== AFTER SALTING ===\n",
      "+---------------------+-----+\n",
      "|BookingStatus_salted |count|\n",
      "+---------------------+-----+\n",
      "|Completed_1          |9447 |\n",
      "|Completed_6          |9442 |\n",
      "|Completed_4          |9325 |\n",
      "|Completed_8          |9310 |\n",
      "|Completed_9          |9301 |\n",
      "|Completed_3          |9286 |\n",
      "|Completed_7          |9253 |\n",
      "|Completed_2          |9248 |\n",
      "|Completed_5          |9228 |\n",
      "|Completed_0          |9160 |\n",
      "|Cancelled by Driver_7|2802 |\n",
      "|Cancelled by Driver_0|2735 |\n",
      "|Cancelled by Driver_5|2720 |\n",
      "|Cancelled by Driver_3|2717 |\n",
      "|Cancelled by Driver_2|2708 |\n",
      "|Cancelled by Driver_9|2695 |\n",
      "|Cancelled by Driver_1|2689 |\n",
      "|Cancelled by Driver_4|2662 |\n",
      "|Cancelled by Driver_6|2638 |\n",
      "|Cancelled by Driver_8|2634 |\n",
      "+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, rand, concat_ws, count\n",
    "\n",
    "# Before salting: check distribution\n",
    "print(\"=== BEFORE SALTING ===\")\n",
    "df.groupBy(\"Booking Status\").agg(count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc()).show(truncate=False)\n",
    "\n",
    "# Add salt\n",
    "salted_df = df.withColumn(\n",
    "    \"BookingStatus_salted\",\n",
    "    concat_ws(\"_\", col(\"Booking Status\"), (rand()*10).cast(\"int\"))  # 10 salts\n",
    ")\n",
    "\n",
    "# After salting: check distribution\n",
    "print(\"=== AFTER SALTING ===\")\n",
    "salted_df.groupBy(\"BookingStatus_salted\").agg(count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa562b41-f020-4dd1-bd3f-a884d9a83162",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial = salted_df.groupBy(\"BookingStatus_salted\").agg(count(\"*\").alias(\"partial_count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f646fb7-54f7-45c1-8029-b87bb0ed5cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|      Booking Status|total_count|\n",
      "+--------------------+-----------+\n",
      "|           Completed|      93000|\n",
      "|     No Driver Found|      10500|\n",
      "| Cancelled by Driver|      27000|\n",
      "|Cancelled by Cust...|      10500|\n",
      "|          Incomplete|       9000|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, sum as _sum\n",
    "\n",
    "# Step 1: GroupBy on salted key (parallelism fixed)\n",
    "partial = salted_df.groupBy(\"BookingStatus_salted\").agg(count(\"*\").alias(\"partial_count\"))\n",
    "\n",
    "# Step 2: Extract original key (remove \"_0\", \"_1\", …)\n",
    "unsalted = partial.withColumn(\"Booking Status\", split(col(\"BookingStatus_salted\"), \"_\")[0])\n",
    "\n",
    "# Step 3: Aggregate again to restore correct totals\n",
    "final = unsalted.groupBy(\"Booking Status\").agg(_sum(\"partial_count\").alias(\"total_count\"))\n",
    "final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae1d608a-ccfd-49b4-bea3-6a4416fde30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Without Salting ===\n",
      "+---------------------+-----+\n",
      "|Booking Status       |count|\n",
      "+---------------------+-----+\n",
      "|Completed            |93000|\n",
      "|Cancelled by Driver  |27000|\n",
      "|No Driver Found      |10500|\n",
      "|Cancelled by Customer|10500|\n",
      "|Incomplete           |9000 |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "print(\"=== Without Salting ===\")\n",
    "df.groupBy(\"Booking Status\") \\\n",
    "  .agg(count(\"*\").alias(\"count\")) \\\n",
    "  .orderBy(\"count\", ascending=False) \\\n",
    "  .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e58f114-cce6-4c70-94c2-19d1304739b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== With Salting ===\n",
      "+---------------------+-----+\n",
      "|Booking Status       |count|\n",
      "+---------------------+-----+\n",
      "|Completed            |93000|\n",
      "|Cancelled by Driver  |27000|\n",
      "|No Driver Found      |10500|\n",
      "|Cancelled by Customer|10500|\n",
      "|Incomplete           |9000 |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, rand, concat_ws, split, sum as _sum\n",
    "\n",
    "# Add salt (10 buckets)\n",
    "salted = df.withColumn(\n",
    "    \"BookingStatus_salted\",\n",
    "    concat_ws(\"_\", col(\"Booking Status\"), (rand()*10).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Partial aggregation (parallelized!)\n",
    "partial = salted.groupBy(\"BookingStatus_salted\") \\\n",
    "    .agg(count(\"*\").alias(\"partial_count\"))\n",
    "\n",
    "# Unsalt (merge back into original key)\n",
    "final = partial.withColumn(\"Booking Status\", split(col(\"BookingStatus_salted\"), \"_\")[0]) \\\n",
    "    .groupBy(\"Booking Status\") \\\n",
    "    .agg(_sum(\"partial_count\").alias(\"count\"))\n",
    "\n",
    "print(\"=== With Salting ===\")\n",
    "final.orderBy(\"count\", ascending=False).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557248d2-3d2a-4d6a-9d48-80e6a0977e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|      Booking Status|count|\n",
      "+--------------------+-----+\n",
      "|Cancelled by Cust...|10500|\n",
      "| Cancelled by Driver|27000|\n",
      "|     No Driver Found|10500|\n",
      "|          Incomplete| 9000|\n",
      "|           Completed|93000|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repartition / Coalesce\n",
    "\n",
    "# Repartition based on skewed column\n",
    "df.repartition(20, \"Booking Status\").groupBy(\"Booking Status\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283910bf-1b55-4564-9ff5-c6ddb66eba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|Booking Value|         total_sum|\n",
      "+-------------+------------------+\n",
      "|      null_44|              NULL|\n",
      "|      null_27|              NULL|\n",
      "|       187_17|             40.04|\n",
      "|       417_14|            131.88|\n",
      "|      1042_23|             25.72|\n",
      "|       713_15|             12.72|\n",
      "|        92_25|             99.03|\n",
      "|       282_23| 96.45000000000002|\n",
      "|      1106_20|             43.19|\n",
      "|       495_17|108.65999999999998|\n",
      "|       284_22| 56.53999999999999|\n",
      "|       842_18|             78.93|\n",
      "|        91_21|             36.39|\n",
      "|       432_17|            113.68|\n",
      "|       390_15|46.330000000000005|\n",
      "|       730_35|             94.63|\n",
      "|       363_11|              49.7|\n",
      "|       202_19|             22.25|\n",
      "|       428_17| 70.22999999999999|\n",
      "|        569_6|             32.75|\n",
      "+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, rand, concat_ws, sum as _sum\n",
    "\n",
    "# Add salt\n",
    "salted = df.withColumn(\"BookingValue_salted\",\n",
    "                       concat_ws(\"_\", col(\"Booking Value\"), (rand()*50).cast(\"int\")))\n",
    "\n",
    "# Partial aggregation\n",
    "partial = salted.groupBy(\"BookingValue_salted\").agg(_sum(\"Ride Distance\").alias(\"partial_sum\"))\n",
    "\n",
    "# Final aggregation (remove salt)\n",
    "final = partial.withColumn(\"Booking Value\", col(\"BookingValue_salted\").substr(0, 10)) \\\n",
    "               .groupBy(\"Booking Value\").agg(_sum(\"partial_sum\").alias(\"total_sum\"))\n",
    "\n",
    "final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a236b33e-f89c-46ca-8ea5-b1add94b92b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|Booking Value|         total_sum|\n",
      "+-------------+------------------+\n",
      "|          691|1575.2400000000002|\n",
      "|          467|3219.9700000000003|\n",
      "|          296| 2866.810000000001|\n",
      "|         4032|             32.11|\n",
      "|         1436|113.60999999999999|\n",
      "|          675|1615.0099999999995|\n",
      "|          829|1447.7599999999998|\n",
      "|         1159|            308.44|\n",
      "|         1090| 699.5000000000001|\n",
      "|         1512|246.42999999999995|\n",
      "|         1572|            248.12|\n",
      "|         2136|56.739999999999995|\n",
      "|         2162|             49.44|\n",
      "|         2294|             21.22|\n",
      "|         2088|             29.09|\n",
      "|          125|           4429.68|\n",
      "|          451|           3484.38|\n",
      "|          944| 952.4700000000001|\n",
      "|          800|           1631.54|\n",
      "|          853|1350.1800000000003|\n",
      "+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Correct unsalting\n",
    "final = partial.withColumn(\n",
    "    \"Booking Value\", split(col(\"BookingValue_salted\"), \"_\")[0]\n",
    ").groupBy(\"Booking Value\").agg(_sum(\"partial_sum\").alias(\"total_sum\"))\n",
    "\n",
    "final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f2e640e-db26-49df-a00a-bb087385253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['BookingStatus], ['BookingStatus, count(1) AS count#3565L]\n",
      "+- LogicalRDD [BookingStatus#3561], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "BookingStatus: string, count: bigint\n",
      "Aggregate [BookingStatus#3561], [BookingStatus#3561, count(1) AS count#3565L]\n",
      "+- LogicalRDD [BookingStatus#3561], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [BookingStatus#3561], [BookingStatus#3561, count(1) AS count#3565L]\n",
      "+- LogicalRDD [BookingStatus#3561], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[BookingStatus#3561], functions=[count(1)], output=[BookingStatus#3561, count#3565L])\n",
      "   +- Exchange hashpartitioning(BookingStatus#3561, 200), ENSURE_REQUIREMENTS, [plan_id=8824]\n",
      "      +- HashAggregate(keys=[BookingStatus#3561], functions=[partial_count(1)], output=[BookingStatus#3561, count#3569L])\n",
      "         +- Scan ExistingRDD[BookingStatus#3561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🔹 What is Adaptive Query Execution (AQE)?\n",
    "\n",
    "# Normally, Spark’s query plan (how it decides to run joins, shuffles, etc.) is decided before execution.\n",
    "# But sometimes Spark guesses wrong because it doesn’t know the real data distribution until it actually runs.\n",
    "\n",
    "# 🔹 What AQE does\n",
    "\n",
    "# AQE has three main features:\n",
    "# Dynamically coalesce shuffle partitions\n",
    "# If Spark creates 200 shuffle partitions, but only 5 of them have data → AQE merges them automatically.\n",
    "# This avoids tiny tasks overhead.\n",
    "# Handle data skew dynamicall\n",
    "# If one partition is much larger than others, AQE can split it into smaller ones (similar to salting, but automatic).\n",
    "# This avoids stragglers (slow tasks due to skew).\n",
    "# Change join strategy at runtime\n",
    "# Spark may think a Sort-Merge Join is needed, but after seeing actual data sizes it realizes a Broadcast Join is faster.\n",
    "# AQE switches automatically.\n",
    "\n",
    "# 🔹 When to use AQE?\n",
    "\n",
    "# When you have skewed data (e.g., one category dominates).\n",
    "\n",
    "# When shuffle partitions are too many/too few.\n",
    "\n",
    "# When data size is hard to estimate before execution.\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", True)\n",
    "\n",
    "from pyspark.sql import Row\n",
    "data = [Row(BookingStatus=\"Completed\") for _ in range(95000)] + \\\n",
    "       [Row(BookingStatus=\"Cancelled\") for _ in range(5000)]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "# Aggregation that triggers shuffle\n",
    "agg = df.groupBy(\"BookingStatus\").count()\n",
    "agg.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f599c30-7595-44f0-a580-e74533c4bb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['BookingStatus], ['BookingStatus, count(1) AS count#3572L]\n",
      "+- LogicalRDD [BookingStatus#3561], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "BookingStatus: string, count: bigint\n",
      "Aggregate [BookingStatus#3561], [BookingStatus#3561, count(1) AS count#3572L]\n",
      "+- LogicalRDD [BookingStatus#3561], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [BookingStatus#3561], [BookingStatus#3561, count(1) AS count#3572L]\n",
      "+- LogicalRDD [BookingStatus#3561], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[BookingStatus#3561], functions=[count(1)], output=[BookingStatus#3561, count#3572L])\n",
      "+- Exchange hashpartitioning(BookingStatus#3561, 200), ENSURE_REQUIREMENTS, [plan_id=8839]\n",
      "   +- *(1) HashAggregate(keys=[BookingStatus#3561], functions=[partial_count(1)], output=[BookingStatus#3561, count#3576L])\n",
      "      +- *(1) Scan ExistingRDD[BookingStatus#3561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "df.groupBy(\"BookingStatus\").count().explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09e6be84-6981-454e-9f3a-0310b45eda15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['BookingStatus], ['BookingStatus, count(1) AS count#3579L]\n",
      "+- LogicalRDD [BookingStatus#3561], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "BookingStatus: string, count: bigint\n",
      "Aggregate [BookingStatus#3561], [BookingStatus#3561, count(1) AS count#3579L]\n",
      "+- LogicalRDD [BookingStatus#3561], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [BookingStatus#3561], [BookingStatus#3561, count(1) AS count#3579L]\n",
      "+- LogicalRDD [BookingStatus#3561], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[BookingStatus#3561], functions=[count(1)], output=[BookingStatus#3561, count#3579L])\n",
      "   +- Exchange hashpartitioning(BookingStatus#3561, 200), ENSURE_REQUIREMENTS, [plan_id=8860]\n",
      "      +- HashAggregate(keys=[BookingStatus#3561], functions=[partial_count(1)], output=[BookingStatus#3561, count#3583L])\n",
      "         +- Scan ExistingRDD[BookingStatus#3561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So the logical/initial plan looks the same — but the key difference is:\n",
    "\n",
    "# ❌ Without AQE → fixed number of shuffle partitions.\n",
    "\n",
    "# ✅ With AQE → partitions can shrink/merge at runtime → fewer, bigger tasks, faster execution\n",
    "# Say you group 100k rows into just 2 categories (Completed, Cancelled):\n",
    "# Without AQE → Spark still spawns 200 shuffle partitions. 198 tasks are empty/useless.\n",
    "# With AQE → Spark will merge down to 2 partitions. Only 2 tasks run after shuffle.\n",
    "# That’s why in your plan you see AdaptiveSparkPlan isFinalPlan=false. When the job finishes, Spark will mark it as isFinalPlan=true with coalesced partitions.\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "df.groupBy(\"BookingStatus\").count().explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12a6dd28-3e17-4bd3-a7f7-1682451014e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Large dataset\n",
    "big = spark.createDataFrame([Row(booking_id=i, status_id=i % 5) for i in range(100000)])\n",
    "\n",
    "# Small dataset\n",
    "small = spark.createDataFrame([\n",
    "    Row(status_id=0, status=\"Completed\"),\n",
    "    Row(status_id=1, status=\"Cancelled by Driver\"),\n",
    "    Row(status_id=2, status=\"Cancelled by Customer\"),\n",
    "    Row(status_id=3, status=\"No Driver Found\"),\n",
    "    Row(status_id=4, status=\"Incomplete\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2002ada-12bc-4085-9dbf-b4b5cd261462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [status_id])\n",
      ":- LogicalRDD [booking_id#3584L, status_id#3585L], false\n",
      "+- LogicalRDD [status_id#3588L, status#3589], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "status_id: bigint, booking_id: bigint, status: string\n",
      "Project [status_id#3585L, booking_id#3584L, status#3589]\n",
      "+- Join Inner, (status_id#3585L = status_id#3588L)\n",
      "   :- LogicalRDD [booking_id#3584L, status_id#3585L], false\n",
      "   +- LogicalRDD [status_id#3588L, status#3589], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [status_id#3585L, booking_id#3584L, status#3589]\n",
      "+- Join Inner, (status_id#3585L = status_id#3588L)\n",
      "   :- Filter isnotnull(status_id#3585L)\n",
      "   :  +- LogicalRDD [booking_id#3584L, status_id#3585L], false\n",
      "   +- Filter isnotnull(status_id#3588L)\n",
      "      +- LogicalRDD [status_id#3588L, status#3589], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [status_id#3585L, booking_id#3584L, status#3589]\n",
      "   +- SortMergeJoin [status_id#3585L], [status_id#3588L], Inner\n",
      "      :- Sort [status_id#3585L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(status_id#3585L, 200), ENSURE_REQUIREMENTS, [plan_id=8887]\n",
      "      :     +- Filter isnotnull(status_id#3585L)\n",
      "      :        +- Scan ExistingRDD[booking_id#3584L,status_id#3585L]\n",
      "      +- Sort [status_id#3588L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(status_id#3588L, 200), ENSURE_REQUIREMENTS, [plan_id=8888]\n",
      "            +- Filter isnotnull(status_id#3588L)\n",
      "               +- Scan ExistingRDD[status_id#3588L,status#3589]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = big.join(small, \"status_id\")\n",
    "joined.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f86ab54-657c-4740-b86c-2231cdc55059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [status_id])\n",
      ":- LogicalRDD [booking_id#3584L, status_id#3585L], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [status_id#3588L, status#3589], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "status_id: bigint, booking_id: bigint, status: string\n",
      "Project [status_id#3585L, booking_id#3584L, status#3589]\n",
      "+- Join Inner, (status_id#3585L = status_id#3588L)\n",
      "   :- LogicalRDD [booking_id#3584L, status_id#3585L], false\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- LogicalRDD [status_id#3588L, status#3589], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [status_id#3585L, booking_id#3584L, status#3589]\n",
      "+- Join Inner, (status_id#3585L = status_id#3588L), rightHint=(strategy=broadcast)\n",
      "   :- Filter isnotnull(status_id#3585L)\n",
      "   :  +- LogicalRDD [booking_id#3584L, status_id#3585L], false\n",
      "   +- Filter isnotnull(status_id#3588L)\n",
      "      +- LogicalRDD [status_id#3588L, status#3589], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [status_id#3585L, booking_id#3584L, status#3589]\n",
      "   +- BroadcastHashJoin [status_id#3585L], [status_id#3588L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(status_id#3585L)\n",
      "      :  +- Scan ExistingRDD[booking_id#3584L,status_id#3585L]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=8917]\n",
      "         +- Filter isnotnull(status_id#3588L)\n",
      "            +- Scan ExistingRDD[status_id#3588L,status#3589]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined_b = big.join(broadcast(small), \"status_id\")\n",
    "joined_b.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a7ed8-156d-47bd-a8ae-35a93b32e93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
