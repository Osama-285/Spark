{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b7ae6a-3b7d-40db-87c5-de8cd9160cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9c811ac2594b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CSV Reads Optimization</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x74c9eb4b2fc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSV Reads Optimization\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e379fff-40ee-453a-8cf5-75ddae67ae9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2024-03-23', Time='12:29:38', Booking ID='\"\"\"CNR5884300\"\"\"', Booking Status='No Driver Found', Customer ID='\"\"\"CID1982111\"\"\"', Vehicle Type='eBike', Pickup Location='Palam Vihar', Drop Location='Jhilmil', Avg VTAT='null', Avg CTAT='null', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='null', Ride Distance='null', Driver Ratings='null', Customer Rating='null', Payment Method='null')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4f7682-f0d6-4e07-ad8e-40b10dd4cc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------+----------+--------+--------------+-----------------+-----------------+-------+-------+-----------------+------------------+----------------------------+------------+----------------------------------------------------------------------------------------------+------------+\n",
      "|column_name                      |data_type  |null_count|null_pct|distinct_count|cardinality_ratio|cardinality_level|min_val|max_val|mean_val         |stddev_val        |percentiles                 |outlier_risk|top_values                                                                                    |quality_flag|\n",
      "+---------------------------------+-----------+----------+--------+--------------+-----------------+-----------------+-------+-------+-----------------+------------------+----------------------------+------------+----------------------------------------------------------------------------------------------+------------+\n",
      "|date                             |categorical|0         |0.0     |384           |0.0026           |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('2024-11-16', 462), ('2024-05-09', 456), ('2024-09-18', 456)]                               |LOW_VARIANCE|\n",
      "|time                             |categorical|0         |0.0     |64158         |0.4277           |mid              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('17:44:57', 16), ('19:17:33', 12), ('18:59:55', 11)]                                        |OK          |\n",
      "|booking_id                       |categorical|0         |0.0     |142132        |0.9475           |high             |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('CNR2726142', 3), ('CNR7908610', 3), ('CNR6337479', 3)]                                     |OK          |\n",
      "|booking_status                   |categorical|0         |0.0     |4             |0.0              |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('Completed', 93000), ('Cancelled by Driver', 27000), ('No Driver Found', 10500)]            |LOW_VARIANCE|\n",
      "|customer_id                      |categorical|0         |0.0     |138782        |0.9252           |high             |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('CID4523979', 3), ('CID6715450', 3), ('CID7828101', 3)]                                     |OK          |\n",
      "|vehicle_type                     |categorical|0         |0.0     |7             |0.0              |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('Auto', 37419), ('Go Mini', 29806), ('Go Sedan', 27141)]                                    |LOW_VARIANCE|\n",
      "|pickup_location                  |categorical|0         |0.0     |179           |0.0012           |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('Khandsa', 949), ('Barakhamba Road', 946), ('Saket', 931)]                                  |LOW_VARIANCE|\n",
      "|drop_location                    |categorical|0         |0.0     |179           |0.0012           |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('Ashram', 936), ('Basai Dhankot', 917), ('Lok Kalyan Marg', 916)]                           |LOW_VARIANCE|\n",
      "|avg_vtat                         |numeric    |0         |0.0     |182           |0.0012           |low              |2.0    |20.0   |8.456351971326171|3.7735638264095708|[5.3, 8.2, 11.2, 14.4, 20.0]|LOW         |None                                                                                          |LOW_VARIANCE|\n",
      "|avg_ctat                         |categorical|0         |0.0     |349           |0.0023           |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 48000), ('24.8', 401), ('25.9', 389)]                                               |LOW_VARIANCE|\n",
      "|cancelled_rides_by_customer      |categorical|0         |0.0     |2             |0.0              |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 139500), ('1', 10500)]                                                              |LOW_VARIANCE|\n",
      "|reason_for_cancelling_by_customer|categorical|0         |0.0     |6             |0.0              |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 139500), ('Wrong Address', 2362), ('Change of plans', 2353)]                        |LOW_VARIANCE|\n",
      "|cancelled_rides_by_driver        |categorical|0         |0.0     |2             |0.0              |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 123000), ('1', 27000)]                                                              |LOW_VARIANCE|\n",
      "|driver_cancellation_reason       |categorical|0         |0.0     |5             |0.0              |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 123000), ('Customer related issue', 6837), ('The customer was coughing/sick', 6751)]|LOW_VARIANCE|\n",
      "|incomplete_rides                 |categorical|0         |0.0     |2             |0.0              |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 141000), ('1', 9000)]                                                               |LOW_VARIANCE|\n",
      "|incomplete_rides_reason          |categorical|0         |0.0     |4             |0.0              |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 141000), ('Customer Demand', 3040), ('Vehicle Breakdown', 3012)]                    |LOW_VARIANCE|\n",
      "|booking_value                    |categorical|0         |0.0     |2465          |0.0164           |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 48000), ('176', 177), ('125', 174)]                                                 |LOW_VARIANCE|\n",
      "|ride_distance                    |categorical|0         |0.0     |5274          |0.0352           |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 48000), ('17.31', 43), ('9.61', 43)]                                                |LOW_VARIANCE|\n",
      "|driver_ratings                   |categorical|0         |0.0     |21            |1.0E-4           |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 57000), ('4.3', 14081), ('4.2', 13841)]                                             |LOW_VARIANCE|\n",
      "|customer_rating                  |categorical|0         |0.0     |21            |1.0E-4           |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 57000), ('4.9', 11642), ('4.6', 11533)]                                             |LOW_VARIANCE|\n",
      "|payment_method                   |categorical|0         |0.0     |6             |0.0              |low              |NULL   |NULL   |NULL             |NULL              |None                        |NULL        |[('null', 48000), ('UPI', 45909), ('Cash', 25367)]                                            |LOW_VARIANCE|\n",
      "+---------------------------------+-----------+----------+--------+--------------+-----------------+-----------------+-------+-------+-----------------+------------------+----------------------------+------------+----------------------------------------------------------------------------------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 57556)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "def clean_col(colname:str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "    \n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "df = df.select([\n",
    "    F.trim(F.regexp_replace(F.col(c), '^\"+|\"+$', '')).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "df.cache()\n",
    "row_count = df.count()\n",
    "\n",
    "exprs = []\n",
    "for c in df.columns:\n",
    "    exprs.append(F.count(F.col(c)).alias(f\"{c}_non_nulls\"))\n",
    "    exprs.append(\n",
    "        F.sum(F.when(F.col(c).cast(\"double\").isNotNull(), 1).otherwise(0))\n",
    "        .alias(f\"{c}_cast_success\")\n",
    "    )\n",
    "\n",
    "stats = df.agg(*exprs).collect()[0]\n",
    "\n",
    "numeric_cols, categorical_cols = [], []\n",
    "for c in df.columns:\n",
    "    non_nulls = stats[f\"{c}_non_nulls\"]\n",
    "    cast_success = stats[f\"{c}_cast_success\"]\n",
    "\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Batch null & distinct stats\n",
    "# ---------------------------------------------------\n",
    "agg_exprs = []\n",
    "for c in df.columns:\n",
    "    agg_exprs.append(\n",
    "        F.sum(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), 1).otherwise(0))\n",
    "        .alias(f\"{c}_nulls\")\n",
    "    )\n",
    "    agg_exprs.append(F.approx_count_distinct(c).alias(f\"{c}_distinct\"))\n",
    "\n",
    "nulls_distinct = df.agg(*agg_exprs).collect()[0].asDict()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Numeric statistics (pre-cast once)\n",
    "# ---------------------------------------------------\n",
    "df_numeric = df.select([F.col(c).cast(\"double\").alias(c) for c in numeric_cols])\n",
    "\n",
    "num_exprs = []\n",
    "for c in numeric_cols:\n",
    "    num_exprs += [\n",
    "        F.min(c).alias(f\"{c}_min\"),\n",
    "        F.max(c).alias(f\"{c}_max\"),\n",
    "        F.mean(c).alias(f\"{c}_mean\"),\n",
    "        F.stddev(c).alias(f\"{c}_stddev\")\n",
    "    ]\n",
    "\n",
    "numeric_stats = df_numeric.agg(*num_exprs).collect()[0].asDict()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Percentiles\n",
    "# ---------------------------------------------------\n",
    "percentiles_dict = {}\n",
    "for c in numeric_cols:\n",
    "    percentiles_dict[c] = df_numeric.na.drop().approxQuantile(\n",
    "        c, [0.25, 0.5, 0.75, 0.95, 0.99], 0.01\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# ðŸ”§ CHANGED: Efficient top categorical values using stack\n",
    "# ---------------------------------------------------\n",
    "if categorical_cols:\n",
    "    stack_expr = F.expr(\n",
    "        \"stack({0}, {1}) as (column_name, value)\".format(\n",
    "            len(categorical_cols),\n",
    "            \", \".join([f\"'{c}', {c}\" for c in categorical_cols])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cat_df = df.select(stack_expr)\n",
    "    freq_df = cat_df.groupBy(\"column_name\", \"value\").count()\n",
    "\n",
    "    w = Window.partitionBy(\"column_name\").orderBy(F.desc(\"count\"))\n",
    "    top_values_df = freq_df.withColumn(\"rank\", F.row_number().over(w)) \\\n",
    "                           .filter(F.col(\"rank\") <= 3)\n",
    "\n",
    "    top_values = {}\n",
    "    for r in top_values_df.collect():\n",
    "        top_values.setdefault(r[\"column_name\"], []).append(\n",
    "            (r[\"value\"], r[\"count\"])\n",
    "        )\n",
    "else:\n",
    "    top_values = {}\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# ðŸ”§ CHANGED: Build final profiling report\n",
    "# ---------------------------------------------------\n",
    "report_rows = []\n",
    "\n",
    "for c in df.columns:\n",
    "    null_count = nulls_distinct[f\"{c}_nulls\"]\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = nulls_distinct[f\"{c}_distinct\"]\n",
    "\n",
    "    # ðŸ”§ CHANGED: Correct naming (this is cardinality, not skew)\n",
    "    cardinality_ratio = round(distinct_count / row_count, 4) if row_count else None\n",
    "\n",
    "    if cardinality_ratio is None:\n",
    "        cardinality_level = \"unknown\"\n",
    "    elif cardinality_ratio < 0.1:\n",
    "        cardinality_level = \"low\"\n",
    "    elif cardinality_ratio < 0.5:\n",
    "        cardinality_level = \"mid\"\n",
    "    else:\n",
    "        cardinality_level = \"high\"\n",
    "\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outlier_risk = None\n",
    "    top_vals = None\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        min_val = numeric_stats.get(f\"{c}_min\")\n",
    "        max_val = numeric_stats.get(f\"{c}_max\")\n",
    "        mean_val = numeric_stats.get(f\"{c}_mean\")\n",
    "        stddev_val = numeric_stats.get(f\"{c}_stddev\")\n",
    "        percentiles = percentiles_dict.get(c)\n",
    "\n",
    "        # ðŸ”§ CHANGED: Outlier RISK instead of expensive count\n",
    "        if mean_val is not None and stddev_val is not None and stddev_val > 0:\n",
    "            outlier_risk = \"HIGH\" if stddev_val > abs(mean_val) else \"LOW\"\n",
    "\n",
    "    else:\n",
    "        top_vals = top_values.get(c)\n",
    "\n",
    "    # ðŸ†• ADDED: Data quality flag\n",
    "    if null_pct is not None and null_pct > 30:\n",
    "        quality_flag = \"HIGH_NULLS\"\n",
    "    elif cardinality_level == \"low\":\n",
    "        quality_flag = \"LOW_VARIANCE\"\n",
    "    else:\n",
    "        quality_flag = \"OK\"\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct,\n",
    "        distinct_count, cardinality_ratio, cardinality_level,\n",
    "        min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outlier_risk, str(top_vals),\n",
    "        quality_flag\n",
    "    ))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7. Final Spark DataFrame\n",
    "# ---------------------------------------------------\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\n",
    "        \"column_name\", \"data_type\",\n",
    "        \"null_count\", \"null_pct\",\n",
    "        \"distinct_count\", \"cardinality_ratio\", \"cardinality_level\",\n",
    "        \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "        \"percentiles\", \"outlier_risk\", \"top_values\",\n",
    "        \"quality_flag\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "report_df.show(100, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e6037a-4b8c-4942-973a-4512e04264f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------+----------+--------+--------------+-----------------+-----------------+----------+------------+-------+-------+-----------------+------------------+---------------------------+------------+----------------------------------------------------------------------------------------------+------------+\n",
      "|column_name                      |data_type  |null_count|null_pct|distinct_count|cardinality_ratio|cardinality_level|skew_score|skew_label  |min_val|max_val|mean_val         |stddev_val        |percentiles                |outlier_risk|top_values                                                                                    |quality_flag|\n",
      "+---------------------------------+-----------+----------+--------+--------------+-----------------+-----------------+----------+------------+-------+-------+-----------------+------------------+---------------------------+------------+----------------------------------------------------------------------------------------------+------------+\n",
      "|date                             |categorical|0         |0.0     |384           |0.0026           |low              |0.0031    |BALANCED    |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('2024-11-16', 462), ('2024-05-09', 456), ('2024-09-18', 456)]                               |LOW_VARIANCE|\n",
      "|time                             |categorical|0         |0.0     |64158         |0.4277           |mid              |1.0E-4    |BALANCED    |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('17:44:57', 16), ('19:17:33', 12), ('18:59:55', 11)]                                        |OK          |\n",
      "|booking_id                       |categorical|0         |0.0     |142132        |0.9475           |high             |0.0       |BALANCED    |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('CNR2726142', 3), ('CNR7908610', 3), ('CNR6337479', 3)]                                     |OK          |\n",
      "|booking_status                   |categorical|0         |0.0     |4             |0.0              |low              |0.62      |SKEWED      |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('Completed', 93000), ('Cancelled by Driver', 27000), ('No Driver Found', 10500)]            |LOW_VARIANCE|\n",
      "|customer_id                      |categorical|0         |0.0     |138782        |0.9252           |high             |0.0       |BALANCED    |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('CID4523979', 3), ('CID6715450', 3), ('CID7828101', 3)]                                     |OK          |\n",
      "|vehicle_type                     |categorical|0         |0.0     |7             |0.0              |low              |0.2495    |BALANCED    |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('Auto', 37419), ('Go Mini', 29806), ('Go Sedan', 27141)]                                    |LOW_VARIANCE|\n",
      "|pickup_location                  |categorical|0         |0.0     |179           |0.0012           |low              |0.0063    |BALANCED    |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('Khandsa', 949), ('Barakhamba Road', 946), ('Saket', 931)]                                  |LOW_VARIANCE|\n",
      "|drop_location                    |categorical|0         |0.0     |179           |0.0012           |low              |0.0062    |BALANCED    |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('Ashram', 936), ('Basai Dhankot', 917), ('Lok Kalyan Marg', 916)]                           |LOW_VARIANCE|\n",
      "|avg_vtat                         |numeric    |0         |0.0     |182           |0.0012           |low              |1.903     |RIGHT_SKEWED|2.0    |20.0   |8.456351971326171|3.7735638264095708|[2.0, 5.3, 8.2, 11.2, 20.0]|LOW         |None                                                                                          |LOW_VARIANCE|\n",
      "|avg_ctat                         |categorical|0         |0.0     |349           |0.0023           |low              |0.32      |SKEWED      |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 48000), ('24.8', 401), ('25.9', 389)]                                               |LOW_VARIANCE|\n",
      "|cancelled_rides_by_customer      |categorical|0         |0.0     |2             |0.0              |low              |0.93      |HOT_KEY_RISK|NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 139500), ('1', 10500)]                                                              |SKEW_RISK   |\n",
      "|reason_for_cancelling_by_customer|categorical|0         |0.0     |6             |0.0              |low              |0.93      |HOT_KEY_RISK|NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 139500), ('Wrong Address', 2362), ('Change of plans', 2353)]                        |SKEW_RISK   |\n",
      "|cancelled_rides_by_driver        |categorical|0         |0.0     |2             |0.0              |low              |0.82      |HIGH_SKEW   |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 123000), ('1', 27000)]                                                              |SKEW_RISK   |\n",
      "|driver_cancellation_reason       |categorical|0         |0.0     |5             |0.0              |low              |0.82      |HIGH_SKEW   |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 123000), ('Customer related issue', 6837), ('The customer was coughing/sick', 6751)]|SKEW_RISK   |\n",
      "|incomplete_rides                 |categorical|0         |0.0     |2             |0.0              |low              |0.94      |HOT_KEY_RISK|NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 141000), ('1', 9000)]                                                               |SKEW_RISK   |\n",
      "|incomplete_rides_reason          |categorical|0         |0.0     |4             |0.0              |low              |0.94      |HOT_KEY_RISK|NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 141000), ('Customer Demand', 3040), ('Vehicle Breakdown', 3012)]                    |SKEW_RISK   |\n",
      "|booking_value                    |categorical|0         |0.0     |2465          |0.0164           |low              |0.32      |SKEWED      |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 48000), ('176', 177), ('125', 174)]                                                 |LOW_VARIANCE|\n",
      "|ride_distance                    |categorical|0         |0.0     |5274          |0.0352           |low              |0.32      |SKEWED      |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 48000), ('17.31', 43), ('9.61', 43)]                                                |LOW_VARIANCE|\n",
      "|driver_ratings                   |categorical|0         |0.0     |21            |1.0E-4           |low              |0.38      |SKEWED      |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 57000), ('4.3', 14081), ('4.2', 13841)]                                             |LOW_VARIANCE|\n",
      "|customer_rating                  |categorical|0         |0.0     |21            |1.0E-4           |low              |0.38      |SKEWED      |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 57000), ('4.9', 11642), ('4.6', 11533)]                                             |LOW_VARIANCE|\n",
      "|payment_method                   |categorical|0         |0.0     |6             |0.0              |low              |0.32      |SKEWED      |NULL   |NULL   |NULL             |NULL              |None                       |NULL        |[('null', 48000), ('UPI', 45909), ('Cash', 25367)]                                            |LOW_VARIANCE|\n",
      "+---------------------------------+-----------+----------+--------+--------------+-----------------+-----------------+----------+------------+-------+-------+-----------------+------------------+---------------------------+------------+----------------------------------------------------------------------------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "# df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Clean string values (single select â†’ optimized)\n",
    "# ---------------------------------------------------\n",
    "df = df.select([\n",
    "    F.trim(F.regexp_replace(F.col(c), '^\"+|\"+$', '')).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "df.cache()\n",
    "row_count = df.count()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Detect numeric vs categorical (efficient)\n",
    "# ---------------------------------------------------\n",
    "exprs = []\n",
    "for c in df.columns:\n",
    "    exprs.append(F.count(F.col(c)).alias(f\"{c}_non_nulls\"))\n",
    "    exprs.append(\n",
    "        F.sum(F.when(F.col(c).cast(\"double\").isNotNull(), 1).otherwise(0))\n",
    "        .alias(f\"{c}_cast_success\")\n",
    "    )\n",
    "\n",
    "stats = df.agg(*exprs).collect()[0]\n",
    "\n",
    "numeric_cols, categorical_cols = [], []\n",
    "for c in df.columns:\n",
    "    non_nulls = stats[f\"{c}_non_nulls\"]\n",
    "    cast_success = stats[f\"{c}_cast_success\"]\n",
    "\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Null & distinct stats (batch)\n",
    "# ---------------------------------------------------\n",
    "agg_exprs = []\n",
    "for c in df.columns:\n",
    "    agg_exprs.append(\n",
    "        F.sum(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), 1).otherwise(0))\n",
    "        .alias(f\"{c}_nulls\")\n",
    "    )\n",
    "    agg_exprs.append(F.approx_count_distinct(c).alias(f\"{c}_distinct\"))\n",
    "\n",
    "nulls_distinct = df.agg(*agg_exprs).collect()[0].asDict()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Numeric statistics (pre-cast once)\n",
    "# ---------------------------------------------------\n",
    "df_numeric = df.select([F.col(c).cast(\"double\").alias(c) for c in numeric_cols])\n",
    "\n",
    "num_exprs = []\n",
    "for c in numeric_cols:\n",
    "    num_exprs += [\n",
    "        F.min(c).alias(f\"{c}_min\"),\n",
    "        F.max(c).alias(f\"{c}_max\"),\n",
    "        F.mean(c).alias(f\"{c}_mean\"),\n",
    "        F.stddev(c).alias(f\"{c}_stddev\")\n",
    "    ]\n",
    "\n",
    "numeric_stats = df_numeric.agg(*num_exprs).collect()[0].asDict()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7. Percentiles (used for skew + outliers)\n",
    "# ---------------------------------------------------\n",
    "percentiles_dict = {}\n",
    "for c in numeric_cols:\n",
    "    percentiles_dict[c] = df_numeric.na.drop().approxQuantile(\n",
    "        c, [0.01, 0.25, 0.5, 0.75, 0.99], 0.01\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 8. Top categorical values (optimized stack)\n",
    "# ---------------------------------------------------\n",
    "if categorical_cols:\n",
    "    stack_expr = F.expr(\n",
    "        \"stack({0}, {1}) as (column_name, value)\".format(\n",
    "            len(categorical_cols),\n",
    "            \", \".join([f\"'{c}', {c}\" for c in categorical_cols])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cat_df = df.select(stack_expr)\n",
    "    freq_df = cat_df.groupBy(\"column_name\", \"value\").count()\n",
    "\n",
    "    w = Window.partitionBy(\"column_name\").orderBy(F.desc(\"count\"))\n",
    "    top_values_df = freq_df.withColumn(\"rank\", F.row_number().over(w)) \\\n",
    "                           .filter(F.col(\"rank\") <= 3)\n",
    "\n",
    "    top_values = {}\n",
    "    for r in top_values_df.collect():\n",
    "        top_values.setdefault(r[\"column_name\"], []).append(\n",
    "            (r[\"value\"], r[\"count\"])\n",
    "        )\n",
    "else:\n",
    "    top_values = {}\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# ðŸ†• 9. SKEW LOGIC (Senior-level)\n",
    "# ---------------------------------------------------\n",
    "skew_info = {}\n",
    "\n",
    "# ---- Categorical skew (hot key detection)\n",
    "for c in categorical_cols:\n",
    "    vals = top_values.get(c)\n",
    "    if vals:\n",
    "        top_count = vals[0][1]\n",
    "        dominance_ratio = round(top_count / row_count, 4) if row_count else None\n",
    "\n",
    "        if dominance_ratio is None:\n",
    "            skew_label = \"unknown\"\n",
    "        elif dominance_ratio > 0.9:\n",
    "            skew_label = \"HOT_KEY_RISK\"\n",
    "        elif dominance_ratio > 0.7:\n",
    "            skew_label = \"HIGH_SKEW\"\n",
    "        elif dominance_ratio > 0.3:\n",
    "            skew_label = \"SKEWED\"\n",
    "        else:\n",
    "            skew_label = \"BALANCED\"\n",
    "\n",
    "        skew_info[c] = (dominance_ratio, skew_label)\n",
    "\n",
    "# ---- Numeric skew (distribution skew)\n",
    "for c in numeric_cols:\n",
    "    p = percentiles_dict.get(c)\n",
    "    if p and len(p) == 5:\n",
    "        p01, p25, p50, p75, p99 = p\n",
    "\n",
    "        if (p50 - p01) != 0:\n",
    "            skew_score = round((p99 - p50) / (p50 - p01), 3)\n",
    "\n",
    "            if skew_score > 1.5:\n",
    "                skew_label = \"RIGHT_SKEWED\"\n",
    "            elif skew_score < 0.7:\n",
    "                skew_label = \"LEFT_SKEWED\"\n",
    "            else:\n",
    "                skew_label = \"SYMMETRIC\"\n",
    "        else:\n",
    "            skew_score, skew_label = None, \"unknown\"\n",
    "\n",
    "        skew_info[c] = (skew_score, skew_label)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 10. Final profiling report\n",
    "# ---------------------------------------------------\n",
    "report_rows = []\n",
    "\n",
    "for c in df.columns:\n",
    "    null_count = nulls_distinct[f\"{c}_nulls\"]\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = nulls_distinct[f\"{c}_distinct\"]\n",
    "\n",
    "    cardinality_ratio = round(distinct_count / row_count, 4) if row_count else None\n",
    "\n",
    "    if cardinality_ratio is None:\n",
    "        cardinality_level = \"unknown\"\n",
    "    elif cardinality_ratio < 0.1:\n",
    "        cardinality_level = \"low\"\n",
    "    elif cardinality_ratio < 0.5:\n",
    "        cardinality_level = \"mid\"\n",
    "    else:\n",
    "        cardinality_level = \"high\"\n",
    "\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outlier_risk = None\n",
    "    top_vals = None\n",
    "\n",
    "    skew_score, skew_label = skew_info.get(c, (None, None))\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        min_val = numeric_stats.get(f\"{c}_min\")\n",
    "        max_val = numeric_stats.get(f\"{c}_max\")\n",
    "        mean_val = numeric_stats.get(f\"{c}_mean\")\n",
    "        stddev_val = numeric_stats.get(f\"{c}_stddev\")\n",
    "        percentiles = percentiles_dict.get(c)\n",
    "\n",
    "        if stddev_val is not None and mean_val is not None:\n",
    "            outlier_risk = \"HIGH\" if stddev_val > abs(mean_val) else \"LOW\"\n",
    "\n",
    "    else:\n",
    "        top_vals = top_values.get(c)\n",
    "\n",
    "    if null_pct is not None and null_pct > 30:\n",
    "        quality_flag = \"HIGH_NULLS\"\n",
    "    elif skew_label in (\"HOT_KEY_RISK\", \"HIGH_SKEW\"):\n",
    "        quality_flag = \"SKEW_RISK\"\n",
    "    elif cardinality_level == \"low\":\n",
    "        quality_flag = \"LOW_VARIANCE\"\n",
    "    else:\n",
    "        quality_flag = \"OK\"\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct,\n",
    "        distinct_count, cardinality_ratio, cardinality_level,\n",
    "        skew_score, skew_label,\n",
    "        min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outlier_risk, str(top_vals),\n",
    "        quality_flag\n",
    "    ))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 11. Final Spark DataFrame\n",
    "# ---------------------------------------------------\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\n",
    "        \"column_name\", \"data_type\",\n",
    "        \"null_count\", \"null_pct\",\n",
    "        \"distinct_count\", \"cardinality_ratio\", \"cardinality_level\",\n",
    "        \"skew_score\", \"skew_label\",\n",
    "        \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "        \"percentiles\", \"outlier_risk\", \"top_values\",\n",
    "        \"quality_flag\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "report_df.show(100, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2812d5-f656-41c7-b97e-f539f10f27b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
