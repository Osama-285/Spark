{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760fe5c4-0d85-4343-8592-58ae25ffbf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://042ea7b70ba9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataProfilingAndQualityPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70a3f07eddf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"DataProfilingAndQualityPipeline\")\n",
    "        # Executor/driver configs\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")  \n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.orc.impl\", \"native\")\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"600\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee30b18-5820-4735-8fc3-0fd1a40579db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "output_path = \"/data/processed/output.parquet\"\n",
    "\n",
    "def cleanColumnName(col_name):\n",
    "    col_name = col_name.strip()\n",
    "\n",
    "    col_name = re.sub(r\"[.\\s\\-]+\", \"_\", col_name)\n",
    "    col_name = re.sub(r\"[^0-9a-zA-Z_]\", \"\", col_name)\n",
    "    col_name = col_name.lower()\n",
    "    col_name = re.sub(r\"^_+|_+$\", \"\", col_name)\n",
    "    col_name = re.sub(r\"_+\", \"_\", col_name)\n",
    "    \n",
    "    return col_name\n",
    "\n",
    "header = spark.sparkContext.textFile(input_path).first().split(\",\")\n",
    "cleaned_headers = [cleanColumnName(h) for h in header]\n",
    "\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=True).toDF(*cleaned_headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "174bfe0a-20e1-420e-aea7-ef4f65651453",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     46\u001b[0m     report_rows\u001b[38;5;241m.\u001b[39mappend((\n\u001b[1;32m     47\u001b[0m         col, col_dtype, null_count, null_pct,\n\u001b[1;32m     48\u001b[0m         distinct_count, skew_ratio, skew_level,\n\u001b[1;32m     49\u001b[0m         min_val, max_val, mean_val, stddev_val, top_values\n\u001b[1;32m     50\u001b[0m     ))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m report_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumn_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnull_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnull_pct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistinct_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskew_ratio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskew_level\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean_val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstddev_val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Step 4: Duplicate Rows Check\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[1;32m     63\u001b[0m duplicate_count \u001b[38;5;241m=\u001b[39m total_rows \u001b[38;5;241m-\u001b[39m df\u001b[38;5;241m.\u001b[39mdropDuplicates()\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:969\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m schema\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
     ]
    }
   ],
   "source": [
    "total_rows = df.count()\n",
    "report_rows = []\n",
    "\n",
    "for col in df.columns:\n",
    "    col_dtype = dict(df.dtypes)[col]\n",
    "    \n",
    "    # Null count\n",
    "    null_count = df.filter(F.col(col).isNull() | (F.col(col) == \"\")).count()\n",
    "    null_pct = round((null_count / total_rows) * 100, 2) if total_rows > 0 else 0.0\n",
    "    \n",
    "    # Distinct count\n",
    "    distinct_count = df.select(col).distinct().count()\n",
    "    \n",
    "    # Skew metric\n",
    "    skew_ratio = distinct_count / total_rows if total_rows > 0 else 0.0\n",
    "    if skew_ratio < 0.3:\n",
    "        skew_level = \"HIGH\"\n",
    "    elif skew_ratio < 0.7:\n",
    "        skew_level = \"MEDIUM\"\n",
    "    else:\n",
    "        skew_level = \"LOW\"\n",
    "    \n",
    "    # Numeric stats\n",
    "    min_val, max_val, mean_val, stddev_val = (None, None, None, None)\n",
    "    if col_dtype in [\"int\", \"double\", \"float\", \"bigint\", \"decimal\"]:\n",
    "        stats = df.select(\n",
    "            F.min(col).alias(\"min\"),\n",
    "            F.max(col).alias(\"max\"),\n",
    "            F.mean(col).alias(\"mean\"),\n",
    "            F.stddev(col).alias(\"stddev\")\n",
    "        ).first()\n",
    "        min_val, max_val, mean_val, stddev_val = stats\n",
    "    \n",
    "    # Top-N (categorical) -> show top 3 most frequent values\n",
    "    top_values = None\n",
    "    if col_dtype == \"string\":\n",
    "        top_vals = (\n",
    "            df.groupBy(col).count()\n",
    "              .orderBy(F.desc(\"count\"))\n",
    "              .limit(3)\n",
    "              .toPandas()\n",
    "              .to_dict(orient=\"records\")\n",
    "        )\n",
    "        top_values = str(top_vals)  # store as string\n",
    "    \n",
    "    report_rows.append((\n",
    "        col, col_dtype, null_count, null_pct,\n",
    "        distinct_count, skew_ratio, skew_level,\n",
    "        min_val, max_val, mean_val, stddev_val, top_values\n",
    "    ))\n",
    "\n",
    "# Convert to DataFrame\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\", \"top_values\"]\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Step 4: Duplicate Rows Check\n",
    "# -----------------------\n",
    "duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "print(f\"Duplicate Rows: {duplicate_count}\")\n",
    "\n",
    "# -----------------------\n",
    "# Step 5: Save Outputs\n",
    "# -----------------------\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "report_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6221fada-3de2-49ff-8cd2-02a305d8af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Columns: ['avg_vtat']\n",
      "Categorical Columns: ['date', 'time', 'booking_id', 'booking_status', 'customer_id', 'vehicle_type', 'pickup_location', 'drop_location', 'avg_ctat', 'cancelled_rides_by_customer', 'reason_for_cancelling_by_customer', 'cancelled_rides_by_driver', 'driver_cancellation_reason', 'incomplete_rides', 'incomplete_rides_reason', 'booking_value', 'ride_distance', 'driver_ratings', 'customer_rating', 'payment_method']\n",
      "+----------+----------+----------------+--------------------+-----------------+------------------+---------------------+-------------------+--------------+--------------+---------------------------------+---------------------------------------+-------------------------------+--------------------------------+----------------------+-----------------------------+-------------------+-------------------+--------------------+---------------------+--------------------+\n",
      "|date_nulls|time_nulls|booking_id_nulls|booking_status_nulls|customer_id_nulls|vehicle_type_nulls|pickup_location_nulls|drop_location_nulls|avg_vtat_nulls|avg_ctat_nulls|cancelled_rides_by_customer_nulls|reason_for_cancelling_by_customer_nulls|cancelled_rides_by_driver_nulls|driver_cancellation_reason_nulls|incomplete_rides_nulls|incomplete_rides_reason_nulls|booking_value_nulls|ride_distance_nulls|driver_ratings_nulls|customer_rating_nulls|payment_method_nulls|\n",
      "+----------+----------+----------------+--------------------+-----------------+------------------+---------------------+-------------------+--------------+--------------+---------------------------------+---------------------------------------+-------------------------------+--------------------------------+----------------------+-----------------------------+-------------------+-------------------+--------------------+---------------------+--------------------+\n",
      "|0         |0         |0               |0                   |0                |0                 |0                    |0                  |0             |0             |0                                |0                                      |0                              |0                               |0                     |0                            |0                  |0                  |0                   |0                    |0                   |\n",
      "+----------+----------+----------------+--------------------+-----------------+------------------+---------------------+-------------------+--------------+--------------+---------------------------------+---------------------------------------+-------------------------------+--------------------------------+----------------------+-----------------------------+-------------------+-------------------+--------------------+---------------------+--------------------+\n",
      "\n",
      "+------------+------------+-----------------+------------------+\n",
      "|avg_vtat_min|avg_vtat_max|    avg_vtat_mean|   avg_vtat_stddev|\n",
      "+------------+------------+-----------------+------------------+\n",
      "|         2.0|        20.0|8.456351971326171|3.7735638264095708|\n",
      "+------------+------------+-----------------+------------------+\n",
      "\n",
      "+----------+-----+\n",
      "|date      |count|\n",
      "+----------+-----+\n",
      "|2024-11-16|462  |\n",
      "|2024-09-18|456  |\n",
      "|2024-05-09|456  |\n",
      "|2024-02-06|452  |\n",
      "|2024-10-12|452  |\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+\n",
      "|time    |count|\n",
      "+--------+-----+\n",
      "|17:44:57|16   |\n",
      "|19:17:33|12   |\n",
      "|17:54:33|11   |\n",
      "|18:59:55|11   |\n",
      "|17:55:08|11   |\n",
      "+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-----+\n",
      "|booking_id      |count|\n",
      "+----------------+-----+\n",
      "|\"\"\"CNR2726142\"\"\"|3    |\n",
      "|\"\"\"CNR5292943\"\"\"|3    |\n",
      "|\"\"\"CNR7199036\"\"\"|3    |\n",
      "|\"\"\"CNR7908610\"\"\"|3    |\n",
      "|\"\"\"CNR9603232\"\"\"|3    |\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------------+-----+\n",
      "|booking_status       |count|\n",
      "+---------------------+-----+\n",
      "|Completed            |93000|\n",
      "|Cancelled by Driver  |27000|\n",
      "|No Driver Found      |10500|\n",
      "|Cancelled by Customer|10500|\n",
      "|Incomplete           |9000 |\n",
      "+---------------------+-----+\n",
      "\n",
      "+----------------+-----+\n",
      "|customer_id     |count|\n",
      "+----------------+-----+\n",
      "|\"\"\"CID7828101\"\"\"|3    |\n",
      "|\"\"\"CID5481002\"\"\"|3    |\n",
      "|\"\"\"CID6468528\"\"\"|3    |\n",
      "|\"\"\"CID4523979\"\"\"|3    |\n",
      "|\"\"\"CID6715450\"\"\"|3    |\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+-----+\n",
      "|vehicle_type |count|\n",
      "+-------------+-----+\n",
      "|Auto         |37419|\n",
      "|Go Mini      |29806|\n",
      "|Go Sedan     |27141|\n",
      "|Bike         |22517|\n",
      "|Premier Sedan|18111|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|pickup_location|count|\n",
      "+---------------+-----+\n",
      "|Khandsa        |949  |\n",
      "|Barakhamba Road|946  |\n",
      "|Saket          |931  |\n",
      "|Badarpur       |921  |\n",
      "|Pragati Maidan |920  |\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|drop_location  |count|\n",
      "+---------------+-----+\n",
      "|Ashram         |936  |\n",
      "|Basai Dhankot  |917  |\n",
      "|Lok Kalyan Marg|916  |\n",
      "|Narsinghpur    |913  |\n",
      "|Kalkaji        |912  |\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+\n",
      "|avg_ctat|count|\n",
      "+--------+-----+\n",
      "|null    |48000|\n",
      "|24.8    |401  |\n",
      "|25.9    |389  |\n",
      "|28.1    |388  |\n",
      "|20.5    |386  |\n",
      "+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------------------+------+\n",
      "|cancelled_rides_by_customer|count |\n",
      "+---------------------------+------+\n",
      "|null                       |139500|\n",
      "|1                          |10500 |\n",
      "+---------------------------+------+\n",
      "\n",
      "+--------------------------------------------+------+\n",
      "|reason_for_cancelling_by_customer           |count |\n",
      "+--------------------------------------------+------+\n",
      "|null                                        |139500|\n",
      "|Wrong Address                               |2362  |\n",
      "|Change of plans                             |2353  |\n",
      "|Driver is not moving towards pickup location|2335  |\n",
      "|Driver asked to cancel                      |2295  |\n",
      "+--------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------------+------+\n",
      "|cancelled_rides_by_driver|count |\n",
      "+-------------------------+------+\n",
      "|null                     |123000|\n",
      "|1                        |27000 |\n",
      "+-------------------------+------+\n",
      "\n",
      "+-----------------------------------+------+\n",
      "|driver_cancellation_reason         |count |\n",
      "+-----------------------------------+------+\n",
      "|null                               |123000|\n",
      "|Customer related issue             |6837  |\n",
      "|The customer was coughing/sick     |6751  |\n",
      "|Personal & Car related issues      |6726  |\n",
      "|More than permitted people in there|6686  |\n",
      "+-----------------------------------+------+\n",
      "\n",
      "+----------------+------+\n",
      "|incomplete_rides|count |\n",
      "+----------------+------+\n",
      "|null            |141000|\n",
      "|1               |9000  |\n",
      "+----------------+------+\n",
      "\n",
      "+-----------------------+------+\n",
      "|incomplete_rides_reason|count |\n",
      "+-----------------------+------+\n",
      "|null                   |141000|\n",
      "|Customer Demand        |3040  |\n",
      "|Vehicle Breakdown      |3012  |\n",
      "|Other Issue            |2948  |\n",
      "+-----------------------+------+\n",
      "\n",
      "+-------------+-----+\n",
      "|booking_value|count|\n",
      "+-------------+-----+\n",
      "|null         |48000|\n",
      "|176          |177  |\n",
      "|125          |174  |\n",
      "|200          |170  |\n",
      "|408          |169  |\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+-----+\n",
      "|ride_distance|count|\n",
      "+-------------+-----+\n",
      "|null         |48000|\n",
      "|9.61         |43   |\n",
      "|17.31        |43   |\n",
      "|14.47        |42   |\n",
      "|3.44         |41   |\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+-----+\n",
      "|driver_ratings|count|\n",
      "+--------------+-----+\n",
      "|null          |57000|\n",
      "|4.3           |14081|\n",
      "|4.2           |13841|\n",
      "|4.6           |9368 |\n",
      "|4.4           |7018 |\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|customer_rating|count|\n",
      "+---------------+-----+\n",
      "|null           |57000|\n",
      "|4.9            |11642|\n",
      "|4.6            |11533|\n",
      "|4.3            |10995|\n",
      "|4.2            |10697|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+-----+\n",
      "|payment_method|count|\n",
      "+--------------+-----+\n",
      "|null          |48000|\n",
      "|UPI           |45909|\n",
      "|Cash          |25367|\n",
      "|Uber Wallet   |12276|\n",
      "|Credit Card   |10209|\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "\n",
    "# 1. Read all columns as STRING\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Clean headers (replace spaces, dots, etc.)\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# 3. Auto-detect numeric columns\n",
    "numeric_cols = []\n",
    "categorical_cols = []\n",
    "\n",
    "for c in df.columns:\n",
    "    # try casting to double\n",
    "    test_col = df.withColumn(c + \"_cast\", F.col(c).cast(\"double\"))\n",
    "    non_nulls = test_col.filter(F.col(c).isNotNull()).count()\n",
    "    cast_success = test_col.filter(F.col(c + \"_cast\").isNotNull()).count()\n",
    "    \n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:  # 90% values are numeric\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "print(\"Numeric Columns:\", numeric_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)\n",
    "\n",
    "# 4. Profiling\n",
    "\n",
    "## --- Nulls ---\n",
    "null_report = df.select([\n",
    "    (F.sum(F.when(F.col(c).isNull() | (F.col(c) == \"\"), 1).otherwise(0))\n",
    "     .alias(c + \"_nulls\"))\n",
    "    for c in df.columns\n",
    "])\n",
    "null_report.show(truncate=False)\n",
    "\n",
    "## --- Numeric profiling ---\n",
    "for c in numeric_cols:\n",
    "    df.select(\n",
    "        F.min(F.col(c).cast(\"double\")).alias(c + \"_min\"),\n",
    "        F.max(F.col(c).cast(\"double\")).alias(c + \"_max\"),\n",
    "        F.mean(F.col(c).cast(\"double\")).alias(c + \"_mean\"),\n",
    "        F.stddev(F.col(c).cast(\"double\")).alias(c + \"_stddev\")\n",
    "    ).show()\n",
    "\n",
    "## --- Categorical profiling ---\n",
    "for c in categorical_cols:\n",
    "    df.groupBy(c).count().orderBy(F.desc(\"count\")).show(5, truncate=False)  # top 5 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d819a98e-ad2e-48f0-89ee-1ed6c9991cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------------------------------------------------------------------------+\n",
      "|column_name                      |data_type  |null_count|null_pct|distinct_count|skew_ratio|skew_level|min_val|max_val|mean_val         |stddev_val        |top_values                                                                                    |\n",
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------------------------------------------------------------------------+\n",
      "|date                             |categorical|0         |0.0     |365           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('2024-11-16', 462), ('2024-09-18', 456), ('2024-05-09', 456)]                               |\n",
      "|time                             |categorical|0         |0.0     |62910         |0.42      |mid       |NULL   |NULL   |NULL             |NULL              |[('17:44:57', 16), ('19:17:33', 12), ('18:59:55', 11)]                                        |\n",
      "|booking_id                       |categorical|0         |0.0     |148767        |0.99      |high      |NULL   |NULL   |NULL             |NULL              |[('\"\"\"CNR2726142\"\"\"', 3), ('\"\"\"CNR7908610\"\"\"', 3), ('\"\"\"CNR7199036\"\"\"', 3)]                   |\n",
      "|booking_status                   |categorical|0         |0.0     |5             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('Completed', 93000), ('Cancelled by Driver', 27000), ('No Driver Found', 10500)]            |\n",
      "|customer_id                      |categorical|0         |0.0     |148788        |0.99      |high      |NULL   |NULL   |NULL             |NULL              |[('\"\"\"CID7828101\"\"\"', 3), ('\"\"\"CID4523979\"\"\"', 3), ('\"\"\"CID6468528\"\"\"', 3)]                   |\n",
      "|vehicle_type                     |categorical|0         |0.0     |7             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('Auto', 37419), ('Go Mini', 29806), ('Go Sedan', 27141)]                                    |\n",
      "|pickup_location                  |categorical|0         |0.0     |176           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('Khandsa', 949), ('Barakhamba Road', 946), ('Saket', 931)]                                  |\n",
      "|drop_location                    |categorical|0         |0.0     |176           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('Ashram', 936), ('Basai Dhankot', 917), ('Lok Kalyan Marg', 916)]                           |\n",
      "|avg_vtat                         |numeric    |0         |0.0     |182           |0.0       |low       |2.0    |20.0   |8.456351971326171|3.7735638264095708|None                                                                                          |\n",
      "|avg_ctat                         |categorical|0         |0.0     |352           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('null', 48000), ('24.8', 401), ('25.9', 389)]                                               |\n",
      "|cancelled_rides_by_customer      |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('null', 139500), ('1', 10500)]                                                              |\n",
      "|reason_for_cancelling_by_customer|categorical|0         |0.0     |6             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('null', 139500), ('Wrong Address', 2362), ('Change of plans', 2353)]                        |\n",
      "|cancelled_rides_by_driver        |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('null', 123000), ('1', 27000)]                                                              |\n",
      "|driver_cancellation_reason       |categorical|0         |0.0     |5             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('null', 123000), ('Customer related issue', 6837), ('The customer was coughing/sick', 6751)]|\n",
      "|incomplete_rides                 |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('null', 141000), ('1', 9000)]                                                               |\n",
      "|incomplete_rides_reason          |categorical|0         |0.0     |4             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('null', 141000), ('Customer Demand', 3040), ('Vehicle Breakdown', 3012)]                    |\n",
      "|booking_value                    |categorical|0         |0.0     |2567          |0.02      |low       |NULL   |NULL   |NULL             |NULL              |[('null', 48000), ('176', 177), ('125', 174)]                                                 |\n",
      "|ride_distance                    |categorical|0         |0.0     |4902          |0.03      |low       |NULL   |NULL   |NULL             |NULL              |[('null', 48000), ('17.31', 43), ('9.61', 43)]                                                |\n",
      "|driver_ratings                   |categorical|0         |0.0     |22            |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('null', 57000), ('4.3', 14081), ('4.2', 13841)]                                             |\n",
      "|customer_rating                  |categorical|0         |0.0     |22            |0.0       |low       |NULL   |NULL   |NULL             |NULL              |[('null', 57000), ('4.9', 11642), ('4.6', 11533)]                                             |\n",
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "\n",
    "# 1. Read as string to avoid schema inference issues\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Clean headers\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# 3. Detect numeric vs categorical\n",
    "numeric_cols, categorical_cols = [], []\n",
    "for c in df.columns:\n",
    "    test_col = df.withColumn(c + \"_cast\", F.col(c).cast(\"double\"))\n",
    "    non_nulls = test_col.filter(F.col(c).isNotNull()).count()\n",
    "    cast_success = test_col.filter(F.col(c + \"_cast\").isNotNull()).count()\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# 4. Build profiling report rows\n",
    "report_rows = []\n",
    "\n",
    "row_count = df.count()\n",
    "\n",
    "for c in df.columns:\n",
    "    # base stats\n",
    "    null_count = df.filter(F.col(c).isNull() | (F.col(c) == \"\")).count()\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count > 0 else None\n",
    "    distinct_count = df.select(c).distinct().count()\n",
    "    skew_ratio = round(distinct_count / row_count, 2) if row_count > 0 else None\n",
    "    if skew_ratio is None:\n",
    "        skew_level = \"unknown\"\n",
    "    elif skew_ratio < 0.1:\n",
    "        skew_level = \"low\"\n",
    "    elif skew_ratio < 0.5:\n",
    "        skew_level = \"mid\"\n",
    "    else:\n",
    "        skew_level = \"high\"\n",
    "\n",
    "    # column type\n",
    "    data_type = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    # numeric stats\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    if c in numeric_cols:\n",
    "        stats = df.select(\n",
    "            F.min(F.col(c).cast(\"double\")).alias(\"min\"),\n",
    "            F.max(F.col(c).cast(\"double\")).alias(\"max\"),\n",
    "            F.mean(F.col(c).cast(\"double\")).alias(\"mean\"),\n",
    "            F.stddev(F.col(c).cast(\"double\")).alias(\"stddev\")\n",
    "        ).collect()[0]\n",
    "        min_val, max_val, mean_val, stddev_val = stats\n",
    "\n",
    "    # top categorical values\n",
    "    top_values = None\n",
    "    if c in categorical_cols:\n",
    "        top_vals = df.groupBy(c).count().orderBy(F.desc(\"count\")).limit(3).collect()\n",
    "        top_values = [(row[c], row[\"count\"]) for row in top_vals]\n",
    "\n",
    "    report_rows.append((\n",
    "        c, data_type, null_count, null_pct, distinct_count,\n",
    "        skew_ratio, skew_level, min_val, max_val, mean_val, stddev_val, str(top_values)\n",
    "    ))\n",
    "\n",
    "# 5. Convert to Spark DataFrame\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\", \"top_values\"]\n",
    ")\n",
    "\n",
    "report_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91ab32-21de-4128-b81d-d50f8b215268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
