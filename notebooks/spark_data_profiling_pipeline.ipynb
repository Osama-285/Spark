{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760fe5c4-0d85-4343-8592-58ae25ffbf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f5838d8c3611:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataProfilingAndQualityPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f95d0748820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"DataProfilingAndQualityPipeline\")\n",
    "        # Executor/driver configs\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")  \n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.orc.impl\", \"native\")\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"600\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee30b18-5820-4735-8fc3-0fd1a40579db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "output_path = \"/data/processed/output.parquet\"\n",
    "\n",
    "def cleanColumnName(col_name):\n",
    "    col_name = col_name.strip()\n",
    "\n",
    "    col_name = re.sub(r\"[.\\s\\-]+\", \"_\", col_name)\n",
    "    col_name = re.sub(r\"[^0-9a-zA-Z_]\", \"\", col_name)\n",
    "    col_name = col_name.lower()\n",
    "    col_name = re.sub(r\"^_+|_+$\", \"\", col_name)\n",
    "    col_name = re.sub(r\"_+\", \"_\", col_name)\n",
    "    \n",
    "    return col_name\n",
    "\n",
    "header = spark.sparkContext.textFile(input_path).first().split(\",\")\n",
    "cleaned_headers = [cleanColumnName(h) for h in header]\n",
    "\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=True).toDF(*cleaned_headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade11cbc-0481-4ee3-9aeb-e7d8d2efdd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------+--------+----------------------------------------------------------------------------------------------+\n",
      "|column_name                      |data_type  |null_count|null_pct|distinct_count|skew_ratio|skew_level|min_val|max_val|mean_val         |stddev_val        |percentiles                 |outliers|top_values                                                                                    |\n",
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------+--------+----------------------------------------------------------------------------------------------+\n",
      "|date                             |categorical|0         |0.0     |365           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('2024-11-16', 462), ('2024-09-18', 456), ('2024-05-09', 456)]                               |\n",
      "|time                             |categorical|0         |0.0     |62910         |0.42      |mid       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('17:44:57', 16), ('19:17:33', 12), ('18:59:55', 11)]                                        |\n",
      "|booking_id                       |categorical|0         |0.0     |148767        |0.99      |high      |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('CNR3648267', 3), ('CNR9603232', 3), ('CNR7908610', 3)]                                     |\n",
      "|booking_status                   |categorical|0         |0.0     |5             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Completed', 93000), ('Cancelled by Driver', 27000), ('No Driver Found', 10500)]            |\n",
      "|customer_id                      |categorical|0         |0.0     |148788        |0.99      |high      |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('CID7828101', 3), ('CID6715450', 3), ('CID6468528', 3)]                                     |\n",
      "|vehicle_type                     |categorical|0         |0.0     |7             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Auto', 37419), ('Go Mini', 29806), ('Go Sedan', 27141)]                                    |\n",
      "|pickup_location                  |categorical|0         |0.0     |176           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Khandsa', 949), ('Barakhamba Road', 946), ('Saket', 931)]                                  |\n",
      "|drop_location                    |categorical|0         |0.0     |176           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Ashram', 936), ('Basai Dhankot', 917), ('Lok Kalyan Marg', 916)]                           |\n",
      "|avg_vtat                         |numeric    |0         |0.0     |182           |0.0       |low       |2.0    |20.0   |8.456351971326171|3.7735638264095708|[5.3, 8.2, 11.2, 14.4, 20.0]|200     |None                                                                                          |\n",
      "|avg_ctat                         |categorical|0         |0.0     |352           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 48000), ('24.8', 401), ('25.9', 389)]                                               |\n",
      "|cancelled_rides_by_customer      |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 139500), ('1', 10500)]                                                              |\n",
      "|reason_for_cancelling_by_customer|categorical|0         |0.0     |6             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 139500), ('Wrong Address', 2362), ('Change of plans', 2353)]                        |\n",
      "|cancelled_rides_by_driver        |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 123000), ('1', 27000)]                                                              |\n",
      "|driver_cancellation_reason       |categorical|0         |0.0     |5             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 123000), ('Customer related issue', 6837), ('The customer was coughing/sick', 6751)]|\n",
      "|incomplete_rides                 |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 141000), ('1', 9000)]                                                               |\n",
      "|incomplete_rides_reason          |categorical|0         |0.0     |4             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 141000), ('Customer Demand', 3040), ('Vehicle Breakdown', 3012)]                    |\n",
      "|booking_value                    |categorical|0         |0.0     |2567          |0.02      |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 48000), ('176', 177), ('125', 174)]                                                 |\n",
      "|ride_distance                    |categorical|0         |0.0     |4902          |0.03      |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 48000), ('17.31', 43), ('9.61', 43)]                                                |\n",
      "|driver_ratings                   |categorical|0         |0.0     |22            |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 57000), ('4.3', 14081), ('4.2', 13841)]                                             |\n",
      "|customer_rating                  |categorical|0         |0.0     |22            |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 57000), ('4.9', 11642), ('4.6', 11533)]                                             |\n",
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------+--------+----------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "\n",
    "# 1. Read raw (string only)\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Clean headers\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# 3. Clean string values (remove triple/double quotes + whitespace)\n",
    "for c in df.columns:\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        F.regexp_replace(F.col(c), '^\"+|\"+$', '')  # remove leading/trailing quotes\n",
    "    ).withColumn(\n",
    "        c,\n",
    "        F.trim(F.col(c))  # strip spaces\n",
    "    )\n",
    "\n",
    "row_count = df.count()\n",
    "\n",
    "# --- Detect numeric vs categorical ---\n",
    "numeric_cols, categorical_cols = [], []\n",
    "for c in df.columns:\n",
    "    tmp = df.withColumn(\"tmp\", F.col(c).cast(\"double\"))\n",
    "    non_nulls = tmp.filter(F.col(c).isNotNull()).count()\n",
    "    cast_success = tmp.filter(F.col(\"tmp\").isNotNull()).count()\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# --- Profiling Report ---\n",
    "report_rows = []\n",
    "\n",
    "for c in df.columns:\n",
    "    null_count = df.filter(F.col(c).isNull() | (F.trim(F.col(c)) == \"\")).count()\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = df.select(c).distinct().count()\n",
    "\n",
    "    # Skew ratio\n",
    "    skew_ratio = round(distinct_count / row_count, 2) if row_count else None\n",
    "    if skew_ratio is None:\n",
    "        skew_level = \"unknown\"\n",
    "    elif skew_ratio < 0.1:\n",
    "        skew_level = \"low\"\n",
    "    elif skew_ratio < 0.5:\n",
    "        skew_level = \"mid\"\n",
    "    else:\n",
    "        skew_level = \"high\"\n",
    "\n",
    "    # Defaults\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outliers = None\n",
    "    top_values = None\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        stats = df.select(\n",
    "            F.min(F.col(c).cast(\"double\")).alias(\"min\"),\n",
    "            F.max(F.col(c).cast(\"double\")).alias(\"max\"),\n",
    "            F.mean(F.col(c).cast(\"double\")).alias(\"mean\"),\n",
    "            F.stddev(F.col(c).cast(\"double\")).alias(\"stddev\")\n",
    "        ).collect()[0]\n",
    "        min_val, max_val, mean_val, stddev_val = stats\n",
    "\n",
    "        # Percentiles / Histogram\n",
    "        percentiles = df.select(F.col(c).cast(\"double\").alias(c)) \\\n",
    "            .na.drop() \\\n",
    "            .approxQuantile(c, [0.25, 0.5, 0.75, 0.95, 0.99], 0.01)\n",
    "\n",
    "        # Outliers = values beyond mean ± 3*stddev\n",
    "        if mean_val is not None and stddev_val is not None:\n",
    "            outliers = df.filter(\n",
    "                (F.col(c).cast(\"double\") > mean_val + 3 * stddev_val) |\n",
    "                (F.col(c).cast(\"double\") < mean_val - 3 * stddev_val)\n",
    "            ).count()\n",
    "\n",
    "    elif c in categorical_cols:\n",
    "        # Top categorical values\n",
    "        top_vals = df.groupBy(c).count().orderBy(F.desc(\"count\")).limit(3).collect()\n",
    "        top_values = [(row[c], row[\"count\"]) for row in top_vals]\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct, distinct_count,\n",
    "        skew_ratio, skew_level, min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outliers, str(top_values)\n",
    "    ))\n",
    "\n",
    "# 5. Convert to Spark DataFrame\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "     \"percentiles\", \"outliers\", \"top_values\"]\n",
    ")\n",
    "\n",
    "report_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7ab3eb-67f5-463e-811a-0a804eb9ddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "|column_name       |data_type  |null_count|null_pct|distinct_count|skew_ratio|skew_level|min_val    |max_val        |mean_val            |stddev_val           |percentiles                                                                |outliers|top_values                                                                    |\n",
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "|company_id        |numeric    |0         |0.0     |2             |0.0       |low       |101.0      |106.0          |101.00422035005016  |0.14520367346398552  |[101.0, 101.0, 101.0, 101.0, 106.0]                                        |106     |None                                                                          |\n",
      "|subscriber_id     |numeric    |0         |0.0     |101403        |0.81      |high      |1.0010001E7|9.9960016069E10|8.626726888253229E10|2.5694738292085804E10|[91200001589.0, 94260021507.0, 94810063889.0, 99080007571.0, 99960016069.0]|10074   |None                                                                          |\n",
      "|acct_id           |numeric    |0         |0.0     |125582        |1.0       |high      |1.1010001E7|9.9960017712E10|4.693043836278176E10|4.68393525873552E10  |[11041366.0, 2000737476.0, 93880001323.0, 99080002319.0, 99960017712.0]    |0       |None                                                                          |\n",
      "|acct_type         |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('Residential', 106912), ('Business', 18670)]                                |\n",
      "|profile_acct      |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('Y', 101048), ('N', 24534)]                                                 |\n",
      "|guarantor_id      |numeric    |0         |0.0     |100768        |0.8       |high      |1.2010001E7|9.996001609E10 |6.08428299397742E10 |4.4597070900784195E10|[12040207.0, 91202000890.0, 93470004421.0, 99080005568.0, 99960016090.0]   |0       |None                                                                          |\n",
      "|profile_id        |numeric    |0         |0.0     |101049        |0.8       |high      |1.1010001E7|9.9960016074E10|6.088895826870933E10|4.45833714192387E10  |[11040522.0, 91201000934.0, 93470004646.0, 99080005638.0, 99960016074.0]   |0       |None                                                                          |\n",
      "|activation_date   |categorical|0         |0.0     |6270          |0.05      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('2/19/2025 0:00', 275), ('9/1/2023 0:00', 237), ('10/7/2024 0:00', 234)]    |\n",
      "|first_inv_date    |categorical|0         |0.0     |3175          |0.03      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('12/31/9999 0:00', 3388), ('4/1/2025 0:00', 1389), ('6/20/2022 0:00', 1322)]|\n",
      "|account_desc      |categorical|0         |0.0     |112713        |0.9       |high      |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('TEST, TEST', 281), ('test, test', 272), ('Test, Test', 228)]               |\n",
      "|pin               |categorical|0         |0.0     |4223          |0.03      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 118311), ('1234', 72), ('0', 58)]                                   |\n",
      "|secret_question   |categorical|35423     |28.21   |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('What is the secret answer?', 90159), (None, 35412), ('', 11)]              |\n",
      "|active_services   |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('N', 67322), ('Y', 58260)]                                                  |\n",
      "|num_active_srv    |numeric    |0         |0.0     |106           |0.0       |low       |0.0        |815.0          |0.6956490579860171  |4.642137591396234    |[0.0, 0.0, 1.0, 1.0, 815.0]                                                |259     |None                                                                          |\n",
      "|num_tempdeact_srv |numeric    |0         |0.0     |8             |0.0       |low       |0.0        |16.0           |0.003631093628067717|0.08653294043234605  |[0.0, 0.0, 0.0, 0.0, 16.0]                                                 |403     |None                                                                          |\n",
      "|num_deact_srv     |numeric    |0         |0.0     |100           |0.0       |low       |0.0        |731.0          |0.5207115669443073  |4.2900634316119115   |[0.0, 0.0, 0.0, 2.0, 731.0]                                                |226     |None                                                                          |\n",
      "|created_by_session|numeric    |0         |0.0     |104629        |0.83      |high      |9.9968791E7|9.9960017609E10|6.119865884294397E10|4.444181670985667E10 |[100830957.0, 91200000969.0, 93890000962.0, 99080002370.0, 99960017609.0]  |0       |None                                                                          |\n",
      "|modify_by_session |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "|logically_deleted |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('N', 125110), ('Y', 472)]                                                   |\n",
      "|row_status        |categorical|0         |0.0     |4             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('C', 124818), ('X', 472), ('N', 286)]                                       |\n",
      "|row_created_time  |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "|row_modify_time   |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql import types as T\n",
    "\n",
    "input_path = \"/opt/data/ac_acct.csv\"\n",
    "\n",
    "# 1. Read raw (string only)\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Clean headers\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# 3. Clean string values (remove extra quotes and whitespace)\n",
    "# for c in df.columns:\n",
    "#     df = df.withColumn(\n",
    "#         c,\n",
    "#         F.regexp_replace(F.col(c), '^\"+|\"+$', '')  # remove leading/trailing quotes\n",
    "#     ).withColumn(\n",
    "#         c,\n",
    "#         F.trim(F.col(c))  # strip spaces\n",
    "#     )\n",
    "\n",
    "# ✅ Rule of thumb\n",
    "# Use select when transforming many columns at once → best performance.\n",
    "# Use withColumn for quick, small transformations (1–2 columns).\n",
    "# For large pipelines → prefer select or selectExpr.\n",
    "\n",
    "df = df.select([\n",
    "    F.trim(F.regexp_replace(F.col(c), '^\"+|\"+$', '')).alias(clean_col(c))\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "# Cache to remove re-read multiple times\n",
    "df.cache()\n",
    "row_count = df.count()\n",
    "\n",
    "# --- Detect numeric vs categorical ---\n",
    "# numeric_cols, categorical_cols = [], []\n",
    "# for c in df.columns:\n",
    "#     tmp = df.withColumn(\"tmp\", F.col(c).cast(\"double\"))\n",
    "#     non_nulls = tmp.filter(F.col(c).isNotNull()).count()\n",
    "#     cast_success = tmp.filter(F.col(\"tmp\").isNotNull()).count()\n",
    "#     if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "#         numeric_cols.append(c)\n",
    "#     else:\n",
    "#         categorical_cols.append(c)\n",
    "\n",
    "exprs = []\n",
    "for c in df.columns:\n",
    "    exprs.append(F.count(F.col(c)).alias(f\"{c}_non_nulls\"))\n",
    "    exprs.append(F.sum(F.when(F.col(c).cast(\"double\").isNotNull(), 1).otherwise(0)).alias(f\"{c}_cast_success\"))\n",
    "\n",
    "stats = df.agg(*exprs).collect()[0]\n",
    "\n",
    "numeric_cols, categorical_cols = [], []\n",
    "\n",
    "for c in df.columns:\n",
    "    non_nulls = stats[f\"{c}_non_nulls\"]\n",
    "    cast_success = stats[f\"{c}_cast_success\"]\n",
    "\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Batch distinct + null counts ---\n",
    "agg_exprs = []\n",
    "for c in df.columns:\n",
    "    agg_exprs.append(\n",
    "        F.sum(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), 1).otherwise(0)).alias(f\"{c}_nulls\")\n",
    "    )\n",
    "    agg_exprs.append(F.countDistinct(c).alias(f\"{c}_distinct\"))\n",
    "\n",
    "nulls_distinct_df = df.agg(*agg_exprs)\n",
    "\n",
    "# --- 2. Batch numeric stats ---\n",
    "agg_exprs = []\n",
    "for c in numeric_cols:\n",
    "    agg_exprs += [\n",
    "        F.min(F.col(c).cast(\"double\")).alias(f\"{c}_min\"),\n",
    "        F.max(F.col(c).cast(\"double\")).alias(f\"{c}_max\"),\n",
    "        F.mean(F.col(c).cast(\"double\")).alias(f\"{c}_mean\"),\n",
    "        F.stddev(F.col(c).cast(\"double\")).alias(f\"{c}_stddev\"),\n",
    "    ]\n",
    "\n",
    "numeric_stats_df = df.agg(*agg_exprs)\n",
    "\n",
    "# Collect results into dicts for easy lookup\n",
    "nulls_distinct = nulls_distinct_df.collect()[0].asDict()\n",
    "numeric_stats = numeric_stats_df.collect()[0].asDict()\n",
    "\n",
    "# --- 3. Percentiles (one pass per numeric col) ---\n",
    "percentiles_dict = {}\n",
    "for c in numeric_cols:\n",
    "    percentiles = df.select(F.col(c).cast(\"double\").alias(c)) \\\n",
    "        .na.drop() \\\n",
    "        .approxQuantile(c, [0.25, 0.5, 0.75, 0.95, 0.99], 0.01)\n",
    "    percentiles_dict[c] = percentiles\n",
    "\n",
    "# --- 4. Top values for categorical cols ---\n",
    "top_values_dict = {}\n",
    "for c in categorical_cols:\n",
    "    top_vals = df.groupBy(c).count().orderBy(F.desc(\"count\")).limit(3).collect()\n",
    "    top_values_dict[c] = [(row[c], row[\"count\"]) for row in top_vals]\n",
    "\n",
    "# --- Build final report ---\n",
    "report_rows = []\n",
    "for c in df.columns:\n",
    "    null_count = nulls_distinct[f\"{c}_nulls\"]\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = nulls_distinct[f\"{c}_distinct\"]\n",
    "\n",
    "    skew_ratio = round(distinct_count / row_count, 2) if row_count else None\n",
    "    if skew_ratio is None:\n",
    "        skew_level = \"unknown\"\n",
    "    elif skew_ratio < 0.1:\n",
    "        skew_level = \"low\"\n",
    "    elif skew_ratio < 0.5:\n",
    "        skew_level = \"mid\"\n",
    "    else:\n",
    "        skew_level = \"high\"\n",
    "\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outliers = None\n",
    "    top_values = None\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        min_val = numeric_stats.get(f\"{c}_min\")\n",
    "        max_val = numeric_stats.get(f\"{c}_max\")\n",
    "        mean_val = numeric_stats.get(f\"{c}_mean\")\n",
    "        stddev_val = numeric_stats.get(f\"{c}_stddev\")\n",
    "        percentiles = percentiles_dict.get(c)\n",
    "\n",
    "        if mean_val is not None and stddev_val is not None:\n",
    "            outliers = df.filter(\n",
    "                (F.col(c).cast(\"double\") > mean_val + 3 * stddev_val) |\n",
    "                (F.col(c).cast(\"double\") < mean_val - 3 * stddev_val)\n",
    "            ).count()\n",
    "\n",
    "    elif c in categorical_cols:\n",
    "        top_values = top_values_dict.get(c)\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct, distinct_count,\n",
    "        skew_ratio, skew_level, min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outliers, str(top_values)\n",
    "    ))\n",
    "\n",
    "# --- Convert to Spark DataFrame ---\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "     \"percentiles\", \"outliers\", \"top_values\"]\n",
    ")\n",
    "\n",
    "report_df.show(100,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8058820f-be2e-41ee-a368-dbd12bed106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95806d0-9a96-4a81-8c61-81b547957631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
