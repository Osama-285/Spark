{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760fe5c4-0d85-4343-8592-58ae25ffbf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f5838d8c3611:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataProfilingAndQualityPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f80fb9ed130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"DataProfilingAndQualityPipeline\")\n",
    "        # Executor/driver configs\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")  \n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.orc.impl\", \"native\")\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"600\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee30b18-5820-4735-8fc3-0fd1a40579db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "# output_path = \"/data/processed/output.parquet\"\n",
    "\n",
    "# def cleanColumnName(col_name):\n",
    "#     col_name = col_name.strip()\n",
    "\n",
    "#     col_name = re.sub(r\"[.\\s\\-]+\", \"_\", col_name)\n",
    "#     col_name = re.sub(r\"[^0-9a-zA-Z_]\", \"\", col_name)\n",
    "#     col_name = col_name.lower()\n",
    "#     col_name = re.sub(r\"^_+|_+$\", \"\", col_name)\n",
    "#     col_name = re.sub(r\"_+\", \"_\", col_name)\n",
    "    \n",
    "#     return col_name\n",
    "\n",
    "# header = spark.sparkContext.textFile(input_path).first().split(\",\")\n",
    "# cleaned_headers = [cleanColumnName(h) for h in header]\n",
    "\n",
    "# df = spark.read.csv(input_path, header=True, inferSchema=True).toDF(*cleaned_headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade11cbc-0481-4ee3-9aeb-e7d8d2efdd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "|column_name       |data_type  |null_count|null_pct|distinct_count|skew_ratio|skew_level|min_val    |max_val        |mean_val            |stddev_val           |percentiles                                                                |outliers|top_values                                                                    |\n",
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "|company_id        |numeric    |0         |0.0     |2             |0.0       |low       |101.0      |106.0          |101.00422035005016  |0.14520367346398552  |[101.0, 101.0, 101.0, 101.0, 106.0]                                        |106     |None                                                                          |\n",
      "|subscriber_id     |numeric    |0         |0.0     |101403        |0.81      |high      |1.0010001E7|9.9960016069E10|8.626726888253229E10|2.5694738292085804E10|[91200001589.0, 94260021507.0, 94810063889.0, 99080007571.0, 99960016069.0]|10074   |None                                                                          |\n",
      "|acct_id           |numeric    |0         |0.0     |125582        |1.0       |high      |1.1010001E7|9.9960017712E10|4.693043836278176E10|4.68393525873552E10  |[11041366.0, 2000737476.0, 93880001323.0, 99080002319.0, 99960017712.0]    |0       |None                                                                          |\n",
      "|acct_type         |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('Residential', 106912), ('Business', 18670)]                                |\n",
      "|profile_acct      |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('Y', 101048), ('N', 24534)]                                                 |\n",
      "|guarantor_id      |numeric    |0         |0.0     |100768        |0.8       |high      |1.2010001E7|9.996001609E10 |6.08428299397742E10 |4.4597070900784195E10|[12040207.0, 91202000890.0, 93470004421.0, 99080005568.0, 99960016090.0]   |0       |None                                                                          |\n",
      "|profile_id        |numeric    |0         |0.0     |101049        |0.8       |high      |1.1010001E7|9.9960016074E10|6.088895826870933E10|4.45833714192387E10  |[11040522.0, 91201000934.0, 93470004646.0, 99080005638.0, 99960016074.0]   |0       |None                                                                          |\n",
      "|activation_date   |categorical|0         |0.0     |6270          |0.05      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('2/19/2025 0:00', 275), ('9/1/2023 0:00', 237), ('10/7/2024 0:00', 234)]    |\n",
      "|first_inv_date    |categorical|0         |0.0     |3175          |0.03      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('12/31/9999 0:00', 3388), ('4/1/2025 0:00', 1389), ('6/20/2022 0:00', 1322)]|\n",
      "|account_desc      |categorical|0         |0.0     |112713        |0.9       |high      |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('TEST, TEST', 281), ('test, test', 272), ('Test, Test', 228)]               |\n",
      "|pin               |categorical|0         |0.0     |4223          |0.03      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 118311), ('1234', 72), ('0', 58)]                                   |\n",
      "|secret_question   |categorical|35423     |28.21   |3             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('What is the secret answer?', 90159), (None, 35412), ('', 11)]              |\n",
      "|active_services   |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('N', 67322), ('Y', 58260)]                                                  |\n",
      "|num_active_srv    |numeric    |0         |0.0     |106           |0.0       |low       |0.0        |815.0          |0.6956490579860171  |4.642137591396234    |[0.0, 0.0, 1.0, 1.0, 815.0]                                                |259     |None                                                                          |\n",
      "|num_tempdeact_srv |numeric    |0         |0.0     |8             |0.0       |low       |0.0        |16.0           |0.003631093628067717|0.08653294043234605  |[0.0, 0.0, 0.0, 0.0, 16.0]                                                 |403     |None                                                                          |\n",
      "|num_deact_srv     |numeric    |0         |0.0     |100           |0.0       |low       |0.0        |731.0          |0.5207115669443073  |4.2900634316119115   |[0.0, 0.0, 0.0, 2.0, 731.0]                                                |226     |None                                                                          |\n",
      "|created_by_session|numeric    |0         |0.0     |104629        |0.83      |high      |9.9968791E7|9.9960017609E10|6.119865884294397E10|4.444181670985667E10 |[100830957.0, 91200000969.0, 93890000962.0, 99080002370.0, 99960017609.0]  |0       |None                                                                          |\n",
      "|modify_by_session |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "|logically_deleted |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('N', 125110), ('Y', 472)]                                                   |\n",
      "|row_status        |categorical|0         |0.0     |4             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('C', 124818), ('X', 472), ('N', 286)]                                       |\n",
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "input_path = \"/opt/data/ac_acct.csv\"\n",
    "\n",
    "# 1. Read raw (string only)\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Clean headers\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# 3. Clean string values (remove triple/double quotes + whitespace)\n",
    "for c in df.columns:\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        F.regexp_replace(F.col(c), '^\"+|\"+$', '')  # remove leading/trailing quotes\n",
    "    ).withColumn(\n",
    "        c,\n",
    "        F.trim(F.col(c))  # strip spaces\n",
    "    )\n",
    "\n",
    "row_count = df.count()\n",
    "\n",
    "# --- Detect numeric vs categorical ---\n",
    "numeric_cols, categorical_cols = [], []\n",
    "for c in df.columns:\n",
    "    tmp = df.withColumn(\"tmp\", F.col(c).cast(\"double\"))\n",
    "    non_nulls = tmp.filter(F.col(c).isNotNull()).count()\n",
    "    cast_success = tmp.filter(F.col(\"tmp\").isNotNull()).count()\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# --- Profiling Report ---\n",
    "report_rows = []\n",
    "\n",
    "for c in df.columns:\n",
    "    null_count = df.filter(F.col(c).isNull() | (F.trim(F.col(c)) == \"\")).count()\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = df.select(c).distinct().count()\n",
    "\n",
    "    # Skew ratio\n",
    "    skew_ratio = round(distinct_count / row_count, 2) if row_count else None\n",
    "    if skew_ratio is None:\n",
    "        skew_level = \"unknown\"\n",
    "    elif skew_ratio < 0.1:\n",
    "        skew_level = \"low\"\n",
    "    elif skew_ratio < 0.5:\n",
    "        skew_level = \"mid\"\n",
    "    else:\n",
    "        skew_level = \"high\"\n",
    "\n",
    "    # Defaults\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outliers = None\n",
    "    top_values = None\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        stats = df.select(\n",
    "            F.min(F.col(c).cast(\"double\")).alias(\"min\"),\n",
    "            F.max(F.col(c).cast(\"double\")).alias(\"max\"),\n",
    "            F.mean(F.col(c).cast(\"double\")).alias(\"mean\"),\n",
    "            F.stddev(F.col(c).cast(\"double\")).alias(\"stddev\")\n",
    "        ).collect()[0]\n",
    "        min_val, max_val, mean_val, stddev_val = stats\n",
    "\n",
    "        # Percentiles / Histogram\n",
    "        percentiles = df.select(F.col(c).cast(\"double\").alias(c)) \\\n",
    "            .na.drop() \\\n",
    "            .approxQuantile(c, [0.25, 0.5, 0.75, 0.95, 0.99], 0.01)\n",
    "\n",
    "        # Outliers = values beyond mean ± 3*stddev\n",
    "        if mean_val is not None and stddev_val is not None:\n",
    "            outliers = df.filter(\n",
    "                (F.col(c).cast(\"double\") > mean_val + 3 * stddev_val) |\n",
    "                (F.col(c).cast(\"double\") < mean_val - 3 * stddev_val)\n",
    "            ).count()\n",
    "\n",
    "    elif c in categorical_cols:\n",
    "        # Top categorical values\n",
    "        top_vals = df.groupBy(c).count().orderBy(F.desc(\"count\")).limit(3).collect()\n",
    "        top_values = [(row[c], row[\"count\"]) for row in top_vals]\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct, distinct_count,\n",
    "        skew_ratio, skew_level, min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outliers, str(top_values)\n",
    "    ))\n",
    "\n",
    "# 5. Convert to Spark DataFrame\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "     \"percentiles\", \"outliers\", \"top_values\"]\n",
    ")\n",
    "\n",
    "report_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af7ab3eb-67f5-463e-811a-0a804eb9ddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "|column_name       |data_type  |null_count|null_pct|distinct_count|skew_ratio|skew_level|min_val    |max_val        |mean_val            |stddev_val           |percentiles                                                                |outliers|top_values                                                                    |\n",
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "|company_id        |numeric    |0         |0.0     |2             |0.0       |low       |101.0      |106.0          |101.00422035005016  |0.14520367346398552  |[101.0, 101.0, 101.0, 101.0, 106.0]                                        |106     |None                                                                          |\n",
      "|subscriber_id     |numeric    |0         |0.0     |101303        |0.81      |high      |1.0010001E7|9.9960016069E10|8.626726888253229E10|2.5694738292085804E10|[91200001589.0, 94260021507.0, 94810063889.0, 99080007571.0, 99960016069.0]|10074   |None                                                                          |\n",
      "|acct_id           |numeric    |0         |0.0     |128000        |1.02      |high      |1.1010001E7|9.9960017712E10|4.693043836278176E10|4.68393525873552E10  |[11041366.0, 2000737476.0, 93880001323.0, 99080002319.0, 99960017712.0]    |0       |None                                                                          |\n",
      "|acct_type         |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('Residential', 106912), ('Business', 18670)]                                |\n",
      "|profile_acct      |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('Y', 101048), ('N', 24534)]                                                 |\n",
      "|guarantor_id      |numeric    |0         |0.0     |101786        |0.81      |high      |1.2010001E7|9.996001609E10 |6.08428299397742E10 |4.4597070900784195E10|[12040207.0, 91202000890.0, 93470004421.0, 99080005568.0, 99960016090.0]   |0       |None                                                                          |\n",
      "|profile_id        |numeric    |0         |0.0     |105982        |0.84      |high      |1.1010001E7|9.9960016074E10|6.088895826870933E10|4.45833714192387E10  |[11040522.0, 91201000934.0, 93470004646.0, 99080005638.0, 99960016074.0]   |0       |None                                                                          |\n",
      "|activation_date   |categorical|0         |0.0     |6191          |0.05      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('2/19/2025 0:00', 275), ('9/1/2023 0:00', 237), ('10/7/2024 0:00', 234)]    |\n",
      "|first_inv_date    |categorical|0         |0.0     |3274          |0.03      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('12/31/9999 0:00', 3388), ('4/1/2025 0:00', 1389), ('6/20/2022 0:00', 1322)]|\n",
      "|account_desc      |categorical|0         |0.0     |109603        |0.87      |high      |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('TEST, TEST', 281), ('test, test', 272), ('Test, Test', 228)]               |\n",
      "|pin               |categorical|0         |0.0     |4150          |0.03      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 118311), ('1234', 72), ('0', 58)]                                   |\n",
      "|secret_question   |categorical|35423     |28.21   |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('What is the secret answer?', 90159), (None, 35412), ('', 11)]              |\n",
      "|active_services   |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('N', 67322), ('Y', 58260)]                                                  |\n",
      "|num_active_srv    |numeric    |0         |0.0     |99            |0.0       |low       |0.0        |815.0          |0.6956490579860171  |4.642137591396234    |[0.0, 0.0, 1.0, 1.0, 815.0]                                                |259     |None                                                                          |\n",
      "|num_tempdeact_srv |numeric    |0         |0.0     |8             |0.0       |low       |0.0        |16.0           |0.003631093628067717|0.08653294043234605  |[0.0, 0.0, 0.0, 0.0, 16.0]                                                 |403     |None                                                                          |\n",
      "|num_deact_srv     |numeric    |0         |0.0     |101           |0.0       |low       |0.0        |731.0          |0.5207115669443073  |4.2900634316119115   |[0.0, 0.0, 0.0, 2.0, 731.0]                                                |226     |None                                                                          |\n",
      "|created_by_session|numeric    |0         |0.0     |104820        |0.83      |high      |9.9968791E7|9.9960017609E10|6.119865884294397E10|4.444181670985667E10 |[100830957.0, 91200000969.0, 93890000962.0, 99080002370.0, 99960017609.0]  |0       |None                                                                          |\n",
      "|modify_by_session |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "|logically_deleted |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('N', 125110), ('Y', 472)]                                                   |\n",
      "|row_status        |categorical|0         |0.0     |4             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('C', 124818), ('X', 472), ('N', 286)]                                       |\n",
      "|row_created_time  |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "|row_modify_time   |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "input_path = \"/opt/data/ac_acct.csv\"\n",
    "\n",
    "# 1. Read raw (string only)\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Clean headers\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# 3. Clean string values (remove extra quotes and whitespace)\n",
    "# for c in df.columns:\n",
    "#     df = df.withColumn(\n",
    "#         c,\n",
    "#         F.regexp_replace(F.col(c), '^\"+|\"+$', '')  # remove leading/trailing quotes\n",
    "#     ).withColumn(\n",
    "#         c,\n",
    "#         F.trim(F.col(c))  # strip spaces\n",
    "#     )\n",
    "\n",
    "# ✅ Rule of thumb\n",
    "# Use select when transforming many columns at once → best performance.\n",
    "# Use withColumn for quick, small transformations (1–2 columns).\n",
    "# For large pipelines → prefer select or selectExpr.\n",
    "\n",
    "df = df.select([\n",
    "    F.trim(F.regexp_replace(F.col(c), '^\"+|\"+$', '')).alias(clean_col(c))\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "# Cache to remove re-read multiple times\n",
    "df.cache()\n",
    "row_count = df.count()\n",
    "\n",
    "# --- Detect numeric vs categorical ---\n",
    "# numeric_cols, categorical_cols = [], []\n",
    "# for c in df.columns:\n",
    "#     tmp = df.withColumn(\"tmp\", F.col(c).cast(\"double\"))\n",
    "#     non_nulls = tmp.filter(F.col(c).isNotNull()).count()\n",
    "#     cast_success = tmp.filter(F.col(\"tmp\").isNotNull()).count()\n",
    "#     if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "#         numeric_cols.append(c)\n",
    "#     else:\n",
    "#         categorical_cols.append(c)\n",
    "\n",
    "exprs = []\n",
    "for c in df.columns:\n",
    "    exprs.append(F.count(F.col(c)).alias(f\"{c}_non_nulls\"))\n",
    "    exprs.append(F.sum(F.when(F.col(c).cast(\"double\").isNotNull(), 1).otherwise(0)).alias(f\"{c}_cast_success\"))\n",
    "\n",
    "stats = df.agg(*exprs).collect()[0]\n",
    "\n",
    "numeric_cols, categorical_cols = [], []\n",
    "\n",
    "for c in df.columns:\n",
    "    non_nulls = stats[f\"{c}_non_nulls\"]\n",
    "    cast_success = stats[f\"{c}_cast_success\"]\n",
    "\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Batch distinct + null counts ---\n",
    "agg_exprs = []\n",
    "for c in df.columns:\n",
    "    agg_exprs.append(\n",
    "        F.sum(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), 1).otherwise(0)).alias(f\"{c}_nulls\")\n",
    "    )\n",
    "    agg_exprs.append(F.approx_count_distinct(c).alias(f\"{c}_distinct\"))\n",
    "\n",
    "nulls_distinct_df = df.agg(*agg_exprs)\n",
    "\n",
    "# --- 2. Batch numeric stats ---\n",
    "# agg_exprs = []\n",
    "# for c in numeric_cols:\n",
    "#     agg_exprs += [\n",
    "#         F.min(F.col(c).cast(\"double\")).alias(f\"{c}_min\"),\n",
    "#         F.max(F.col(c).cast(\"double\")).alias(f\"{c}_max\"),\n",
    "#         F.mean(F.col(c).cast(\"double\")).alias(f\"{c}_mean\"),\n",
    "#         F.stddev(F.col(c).cast(\"double\")).alias(f\"{c}_stddev\"),\n",
    "#     ]\n",
    "\n",
    "# numeric_stats_df = df.agg(*agg_exprs)\n",
    "\n",
    "# --- 2. Optimized numeric stats (pre-cast once) ---\n",
    "df_numeric = df.select([F.col(c).cast(\"double\").alias(c) for c in numeric_cols])\n",
    "\n",
    "agg_exprs = []\n",
    "for c in numeric_cols:\n",
    "    agg_exprs += [\n",
    "        F.min(c).alias(f\"{c}_min\"),\n",
    "        F.max(c).alias(f\"{c}_max\"),\n",
    "        F.mean(c).alias(f\"{c}_mean\"),\n",
    "        F.stddev(c).alias(f\"{c}_stddev\")\n",
    "    ]\n",
    "\n",
    "numeric_stats_df = df_numeric.agg(*agg_exprs)\n",
    "numeric_stats_raw = numeric_stats_df.collect()[0].asDict()\n",
    "\n",
    "numeric_stats = {}\n",
    "for c in numeric_cols:\n",
    "    numeric_stats[c] = {\n",
    "        \"min\": numeric_stats_raw[f\"{c}_min\"],\n",
    "        \"max\": numeric_stats_raw[f\"{c}_max\"],\n",
    "        \"mean\": numeric_stats_raw[f\"{c}_mean\"],\n",
    "        \"stddev\": numeric_stats_raw[f\"{c}_stddev\"]\n",
    "    }\n",
    "\n",
    "# Collect results into dicts for easy lookup\n",
    "nulls_distinct = nulls_distinct_df.collect()[0].asDict()\n",
    "numeric_stats = numeric_stats_df.collect()[0].asDict()\n",
    "\n",
    "# --- 3. Percentiles (one pass per numeric col) ---\n",
    "percentiles_dict = {}\n",
    "for c in numeric_cols:\n",
    "    percentiles = df.select(F.col(c).cast(\"double\").alias(c)) \\\n",
    "        .na.drop() \\\n",
    "        .approxQuantile(c, [0.25, 0.5, 0.75, 0.95, 0.99], 0.01)\n",
    "    percentiles_dict[c] = percentiles\n",
    "\n",
    "# --- 4. Top values for categorical cols ---\n",
    "# top_values_dict = {}\n",
    "# for c in categorical_cols:\n",
    "#     top_vals = df.groupBy(c).count().orderBy(F.desc(\"count\")).limit(3).collect()\n",
    "#     top_values_dict[c] = [(row[c], row[\"count\"]) for row in top_vals]\n",
    "\n",
    "cat_expr = F.expr(\"stack({0}, {1}) as (column_name, value)\".format(\n",
    "    len(categorical_cols),\n",
    "    \", \".join([f\"'{c}', {c}\" for c in categorical_cols])\n",
    "))\n",
    "cat_df = df.select(cat_expr)\n",
    "\n",
    "freqs = cat_df.groupBy(\"column_name\", \"value\").count()\n",
    "\n",
    "w = Window.partitionBy(\"column_name\").orderBy(F.desc(\"count\"))\n",
    "top_values_df = freqs.withColumn(\"rank\", F.row_number().over(w)) \\\n",
    "                     .filter(F.col(\"rank\") <= 3)\n",
    "\n",
    "top_values_dict = {}\n",
    "for row in top_values_df.collect():\n",
    "    col = row[\"column_name\"]\n",
    "    if col not in top_values_dict:\n",
    "        top_values_dict[col] = []\n",
    "    top_values_dict[col].append((row[\"value\"], row[\"count\"]))\n",
    "    \n",
    "\n",
    "# --- Build final report ---\n",
    "report_rows = []\n",
    "for c in df.columns:\n",
    "    null_count = nulls_distinct[f\"{c}_nulls\"]\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = nulls_distinct[f\"{c}_distinct\"]\n",
    "\n",
    "    skew_ratio = round(distinct_count / row_count, 2) if row_count else None\n",
    "    if skew_ratio is None:\n",
    "        skew_level = \"unknown\"\n",
    "    elif skew_ratio < 0.1:\n",
    "        skew_level = \"low\"\n",
    "    elif skew_ratio < 0.5:\n",
    "        skew_level = \"mid\"\n",
    "    else:\n",
    "        skew_level = \"high\"\n",
    "\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outliers = None\n",
    "    top_values = None\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        min_val = numeric_stats.get(f\"{c}_min\")\n",
    "        max_val = numeric_stats.get(f\"{c}_max\")\n",
    "        mean_val = numeric_stats.get(f\"{c}_mean\")\n",
    "        stddev_val = numeric_stats.get(f\"{c}_stddev\")\n",
    "        percentiles = percentiles_dict.get(c)\n",
    "\n",
    "        if mean_val is not None and stddev_val is not None:\n",
    "            outliers = df.filter(\n",
    "                (F.col(c).cast(\"double\") > mean_val + 3 * stddev_val) |\n",
    "                (F.col(c).cast(\"double\") < mean_val - 3 * stddev_val)\n",
    "            ).count()\n",
    "\n",
    "    elif c in categorical_cols:\n",
    "        top_values = top_values_dict.get(c)\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct, distinct_count,\n",
    "        skew_ratio, skew_level, min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outliers, str(top_values)\n",
    "    ))\n",
    "\n",
    "# --- Convert to Spark DataFrame ---\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "     \"percentiles\", \"outliers\", \"top_values\"]\n",
    ")\n",
    "\n",
    "report_df.show(100,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8058820f-be2e-41ee-a368-dbd12bed106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95806d0-9a96-4a81-8c61-81b547957631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "|column_name       |data_type  |null_count|null_pct|distinct_count|skew_ratio|skew_level|min_val    |max_val        |mean_val            |stddev_val           |percentiles                                                                |outliers|top_values                                                                    |\n",
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "|company_id        |numeric    |0         |0.0     |2             |0.0       |low       |101.0      |106.0          |101.00422035005016  |0.14520367346398552  |[101.0, 101.0, 101.0, 101.0, 106.0]                                        |106     |None                                                                          |\n",
      "|subscriber_id     |numeric    |0         |0.0     |101303        |0.81      |high      |1.0010001E7|9.9960016069E10|8.626726888253229E10|2.5694738292085804E10|[91200001589.0, 94260021507.0, 94810063889.0, 99080007571.0, 99960016069.0]|10074   |None                                                                          |\n",
      "|acct_id           |numeric    |0         |0.0     |128000        |1.02      |high      |1.1010001E7|9.9960017712E10|4.693043836278176E10|4.68393525873552E10  |[11041366.0, 2000737476.0, 93880001323.0, 99080002319.0, 99960017712.0]    |0       |None                                                                          |\n",
      "|acct_type         |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('Residential', 106912), ('Business', 18670)]                                |\n",
      "|profile_acct      |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('Y', 101048), ('N', 24534)]                                                 |\n",
      "|guarantor_id      |numeric    |0         |0.0     |101786        |0.81      |high      |1.2010001E7|9.996001609E10 |6.08428299397742E10 |4.4597070900784195E10|[12040207.0, 91202000890.0, 93470004421.0, 99080005568.0, 99960016090.0]   |0       |None                                                                          |\n",
      "|profile_id        |numeric    |0         |0.0     |105982        |0.84      |high      |1.1010001E7|9.9960016074E10|6.088895826870933E10|4.45833714192387E10  |[11040522.0, 91201000934.0, 93470004646.0, 99080005638.0, 99960016074.0]   |0       |None                                                                          |\n",
      "|activation_date   |categorical|0         |0.0     |6191          |0.05      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('2/19/2025 0:00', 275), ('9/1/2023 0:00', 237), ('10/7/2024 0:00', 234)]    |\n",
      "|first_inv_date    |categorical|0         |0.0     |3274          |0.03      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('12/31/9999 0:00', 3388), ('4/1/2025 0:00', 1389), ('6/20/2022 0:00', 1322)]|\n",
      "|account_desc      |categorical|0         |0.0     |109603        |0.87      |high      |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('TEST, TEST', 281), ('test, test', 272), ('Test, Test', 228)]               |\n",
      "|pin               |categorical|0         |0.0     |4150          |0.03      |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 118311), ('1234', 72), ('0', 58)]                                   |\n",
      "|secret_question   |categorical|35423     |28.21   |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('What is the secret answer?', 90159), (None, 35412), ('', 11)]              |\n",
      "|active_services   |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('N', 67322), ('Y', 58260)]                                                  |\n",
      "|num_active_srv    |numeric    |0         |0.0     |99            |0.0       |low       |0.0        |815.0          |0.6956490579860171  |4.642137591396234    |[0.0, 0.0, 1.0, 1.0, 815.0]                                                |259     |None                                                                          |\n",
      "|num_tempdeact_srv |numeric    |0         |0.0     |8             |0.0       |low       |0.0        |16.0           |0.003631093628067717|0.08653294043234605  |[0.0, 0.0, 0.0, 0.0, 16.0]                                                 |403     |None                                                                          |\n",
      "|num_deact_srv     |numeric    |0         |0.0     |101           |0.0       |low       |0.0        |731.0          |0.5207115669443073  |4.2900634316119115   |[0.0, 0.0, 0.0, 2.0, 731.0]                                                |226     |None                                                                          |\n",
      "|created_by_session|numeric    |0         |0.0     |104820        |0.83      |high      |9.9968791E7|9.9960017609E10|6.119865884294397E10|4.444181670985667E10 |[100830957.0, 91200000969.0, 93890000962.0, 99080002370.0, 99960017609.0]  |0       |None                                                                          |\n",
      "|modify_by_session |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "|logically_deleted |categorical|0         |0.0     |2             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('N', 125110), ('Y', 472)]                                                   |\n",
      "|row_status        |categorical|0         |0.0     |4             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('C', 124818), ('X', 472), ('N', 286)]                                       |\n",
      "|row_created_time  |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "|row_modify_time   |categorical|0         |0.0     |1             |0.0       |low       |NULL       |NULL           |NULL                |NULL                 |None                                                                       |NULL    |[('NULL', 125582)]                                                            |\n",
      "+------------------+-----------+----------+--------+--------------+----------+----------+-----------+---------------+--------------------+---------------------+---------------------------------------------------------------------------+--------+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############################################################################################################################\n",
    "########################################### NEW VERSION ####################################################################\n",
    "##############################################################################################################################\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "input_path = \"/opt/data/ac_acct.csv\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Read raw CSV as string\n",
    "# -------------------------------\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Clean headers\n",
    "# -------------------------------\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Clean string values (remove quotes & spaces)\n",
    "# -------------------------------\n",
    "df = df.select([\n",
    "    F.trim(F.regexp_replace(F.col(c), '^\"+|\"+$', '')).alias(clean_col(c))\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "# Cache for repeated usage\n",
    "df.cache()\n",
    "row_count = df.count()\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Detect numeric vs categorical columns\n",
    "# -------------------------------\n",
    "exprs = []\n",
    "for c in df.columns:\n",
    "    exprs.append(F.count(F.col(c)).alias(f\"{c}_non_nulls\"))\n",
    "    exprs.append(F.sum(F.when(F.col(c).cast(\"double\").isNotNull(), 1).otherwise(0)).alias(f\"{c}_cast_success\"))\n",
    "\n",
    "stats = df.agg(*exprs).collect()[0]\n",
    "\n",
    "numeric_cols, categorical_cols = [], []\n",
    "for c in df.columns:\n",
    "    non_nulls = stats[f\"{c}_non_nulls\"]\n",
    "    cast_success = stats[f\"{c}_cast_success\"]\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Batch distinct counts + null counts\n",
    "# -------------------------------\n",
    "agg_exprs = []\n",
    "for c in df.columns:\n",
    "    # agg_exprs.append(F.sum(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), 1)).alias(f\"{c}_nulls\"))\n",
    "    agg_exprs.append(\n",
    "    F.sum(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), 1).otherwise(0)).alias(f\"{c}_nulls\"))\n",
    "\n",
    "    agg_exprs.append(F.approx_count_distinct(c).alias(f\"{c}_distinct\"))\n",
    "\n",
    "nulls_distinct_df = df.agg(*agg_exprs)\n",
    "nulls_distinct = nulls_distinct_df.collect()[0].asDict()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Optimized numeric stats (min, max, mean, stddev)\n",
    "# -------------------------------\n",
    "# Pre-cast numeric columns once\n",
    "df_numeric = df.select([F.col(c).cast(\"double\").alias(c) for c in numeric_cols])\n",
    "\n",
    "agg_exprs = []\n",
    "for c in numeric_cols:\n",
    "    agg_exprs += [\n",
    "        F.min(c).alias(f\"{c}_min\"),\n",
    "        F.max(c).alias(f\"{c}_max\"),\n",
    "        F.mean(c).alias(f\"{c}_mean\"),\n",
    "        F.stddev(c).alias(f\"{c}_stddev\")\n",
    "    ]\n",
    "\n",
    "numeric_stats_df = df_numeric.agg(*agg_exprs)\n",
    "numeric_stats_raw = numeric_stats_df.collect()[0].asDict()\n",
    "numeric_stats = {}\n",
    "for c in numeric_cols:\n",
    "    numeric_stats[c] = {\n",
    "        \"min\": numeric_stats_raw[f\"{c}_min\"],\n",
    "        \"max\": numeric_stats_raw[f\"{c}_max\"],\n",
    "        \"mean\": numeric_stats_raw[f\"{c}_mean\"],\n",
    "        \"stddev\": numeric_stats_raw[f\"{c}_stddev\"]\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Percentiles per numeric column\n",
    "# -------------------------------\n",
    "percentiles_dict = {}\n",
    "for c in numeric_cols:\n",
    "    percentiles = df.select(F.col(c).cast(\"double\").alias(c)) \\\n",
    "                    .na.drop() \\\n",
    "                    .approxQuantile(c, [0.25, 0.5, 0.75, 0.95, 0.99], 0.01)\n",
    "    percentiles_dict[c] = percentiles\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Optimized outlier counts (all numeric columns in one pass)\n",
    "# -------------------------------\n",
    "thresholds = {\n",
    "    c: (\n",
    "        numeric_stats[c][\"mean\"] - 3 * numeric_stats[c][\"stddev\"],\n",
    "        numeric_stats[c][\"mean\"] + 3 * numeric_stats[c][\"stddev\"]\n",
    "    )\n",
    "    for c in numeric_cols\n",
    "    if numeric_stats[c][\"mean\"] is not None and numeric_stats[c][\"stddev\"] is not None\n",
    "}\n",
    "\n",
    "outlier_exprs = [\n",
    "    F.sum(F.when((F.col(c) < low) | (F.col(c) > high), 1).otherwise(0)).alias(f\"{c}_outliers\")\n",
    "    for c, (low, high) in thresholds.items()\n",
    "]\n",
    "\n",
    "outliers_df = df_numeric.agg(*outlier_exprs)\n",
    "outliers_raw = outliers_df.collect()[0].asDict()\n",
    "outliers_dict = {c: outliers_raw[f\"{c}_outliers\"] for c in thresholds}\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Optimized top values for categorical columns\n",
    "# -------------------------------\n",
    "if categorical_cols:\n",
    "    cat_expr = F.expr(\"stack({0}, {1}) as (column_name, value)\".format(\n",
    "        len(categorical_cols),\n",
    "        \", \".join([f\"'{c}', {c}\" for c in categorical_cols])\n",
    "    ))\n",
    "    cat_df = df.select(cat_expr)\n",
    "\n",
    "    freqs = cat_df.groupBy(\"column_name\", \"value\").count()\n",
    "    w = Window.partitionBy(\"column_name\").orderBy(F.desc(\"count\"))\n",
    "    top_values_df = freqs.withColumn(\"rank\", F.row_number().over(w)) \\\n",
    "                         .filter(F.col(\"rank\") <= 3)\n",
    "\n",
    "    top_values_dict = {}\n",
    "    for row in top_values_df.collect():\n",
    "        col = row[\"column_name\"]\n",
    "        if col not in top_values_dict:\n",
    "            top_values_dict[col] = []\n",
    "        top_values_dict[col].append((row[\"value\"], row[\"count\"]))\n",
    "else:\n",
    "    top_values_dict = {}\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Build final profiling report\n",
    "# -------------------------------\n",
    "report_rows = []\n",
    "for c in df.columns:\n",
    "    null_count = nulls_distinct[f\"{c}_nulls\"]\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = nulls_distinct[f\"{c}_distinct\"]\n",
    "\n",
    "    skew_ratio = round(distinct_count / row_count, 2) if row_count else None\n",
    "    if skew_ratio is None:\n",
    "        skew_level = \"unknown\"\n",
    "    elif skew_ratio < 0.1:\n",
    "        skew_level = \"low\"\n",
    "    elif skew_ratio < 0.5:\n",
    "        skew_level = \"mid\"\n",
    "    else:\n",
    "        skew_level = \"high\"\n",
    "\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outliers = None\n",
    "    top_values = None\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        stats = numeric_stats.get(c, {})\n",
    "        min_val = stats.get(\"min\")\n",
    "        max_val = stats.get(\"max\")\n",
    "        mean_val = stats.get(\"mean\")\n",
    "        stddev_val = stats.get(\"stddev\")\n",
    "        percentiles = percentiles_dict.get(c)\n",
    "        outliers = outliers_dict.get(c, None)\n",
    "\n",
    "    elif c in categorical_cols:\n",
    "        top_values = top_values_dict.get(c)\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct, distinct_count,\n",
    "        skew_ratio, skew_level, min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outliers, str(top_values)\n",
    "    ))\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Convert to Spark DataFrame and show\n",
    "# -------------------------------\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "     \"percentiles\", \"outliers\", \"top_values\"]\n",
    ")\n",
    "\n",
    "report_df.show(100, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db140a0-a4d7-4d16-8fda-428bc34dae0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
