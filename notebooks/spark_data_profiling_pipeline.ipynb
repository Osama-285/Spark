{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760fe5c4-0d85-4343-8592-58ae25ffbf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f5838d8c3611:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataProfilingAndQualityPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f95d0748820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"DataProfilingAndQualityPipeline\")\n",
    "        # Executor/driver configs\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")  \n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.orc.impl\", \"native\")\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"600\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee30b18-5820-4735-8fc3-0fd1a40579db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "output_path = \"/data/processed/output.parquet\"\n",
    "\n",
    "def cleanColumnName(col_name):\n",
    "    col_name = col_name.strip()\n",
    "\n",
    "    col_name = re.sub(r\"[.\\s\\-]+\", \"_\", col_name)\n",
    "    col_name = re.sub(r\"[^0-9a-zA-Z_]\", \"\", col_name)\n",
    "    col_name = col_name.lower()\n",
    "    col_name = re.sub(r\"^_+|_+$\", \"\", col_name)\n",
    "    col_name = re.sub(r\"_+\", \"_\", col_name)\n",
    "    \n",
    "    return col_name\n",
    "\n",
    "header = spark.sparkContext.textFile(input_path).first().split(\",\")\n",
    "cleaned_headers = [cleanColumnName(h) for h in header]\n",
    "\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=True).toDF(*cleaned_headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade11cbc-0481-4ee3-9aeb-e7d8d2efdd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------+--------+----------------------------------------------------------------------------------------------+\n",
      "|column_name                      |data_type  |null_count|null_pct|distinct_count|skew_ratio|skew_level|min_val|max_val|mean_val         |stddev_val        |percentiles                 |outliers|top_values                                                                                    |\n",
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------+--------+----------------------------------------------------------------------------------------------+\n",
      "|date                             |categorical|0         |0.0     |365           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('2024-11-16', 462), ('2024-09-18', 456), ('2024-05-09', 456)]                               |\n",
      "|time                             |categorical|0         |0.0     |62910         |0.42      |mid       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('17:44:57', 16), ('19:17:33', 12), ('18:59:55', 11)]                                        |\n",
      "|booking_id                       |categorical|0         |0.0     |148767        |0.99      |high      |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('CNR3648267', 3), ('CNR9603232', 3), ('CNR7908610', 3)]                                     |\n",
      "|booking_status                   |categorical|0         |0.0     |5             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Completed', 93000), ('Cancelled by Driver', 27000), ('No Driver Found', 10500)]            |\n",
      "|customer_id                      |categorical|0         |0.0     |148788        |0.99      |high      |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('CID7828101', 3), ('CID6715450', 3), ('CID6468528', 3)]                                     |\n",
      "|vehicle_type                     |categorical|0         |0.0     |7             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Auto', 37419), ('Go Mini', 29806), ('Go Sedan', 27141)]                                    |\n",
      "|pickup_location                  |categorical|0         |0.0     |176           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Khandsa', 949), ('Barakhamba Road', 946), ('Saket', 931)]                                  |\n",
      "|drop_location                    |categorical|0         |0.0     |176           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Ashram', 936), ('Basai Dhankot', 917), ('Lok Kalyan Marg', 916)]                           |\n",
      "|avg_vtat                         |numeric    |0         |0.0     |182           |0.0       |low       |2.0    |20.0   |8.456351971326171|3.7735638264095708|[5.3, 8.2, 11.2, 14.4, 20.0]|200     |None                                                                                          |\n",
      "|avg_ctat                         |categorical|0         |0.0     |352           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 48000), ('24.8', 401), ('25.9', 389)]                                               |\n",
      "|cancelled_rides_by_customer      |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 139500), ('1', 10500)]                                                              |\n",
      "|reason_for_cancelling_by_customer|categorical|0         |0.0     |6             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 139500), ('Wrong Address', 2362), ('Change of plans', 2353)]                        |\n",
      "|cancelled_rides_by_driver        |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 123000), ('1', 27000)]                                                              |\n",
      "|driver_cancellation_reason       |categorical|0         |0.0     |5             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 123000), ('Customer related issue', 6837), ('The customer was coughing/sick', 6751)]|\n",
      "|incomplete_rides                 |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 141000), ('1', 9000)]                                                               |\n",
      "|incomplete_rides_reason          |categorical|0         |0.0     |4             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 141000), ('Customer Demand', 3040), ('Vehicle Breakdown', 3012)]                    |\n",
      "|booking_value                    |categorical|0         |0.0     |2567          |0.02      |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 48000), ('176', 177), ('125', 174)]                                                 |\n",
      "|ride_distance                    |categorical|0         |0.0     |4902          |0.03      |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 48000), ('17.31', 43), ('9.61', 43)]                                                |\n",
      "|driver_ratings                   |categorical|0         |0.0     |22            |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 57000), ('4.3', 14081), ('4.2', 13841)]                                             |\n",
      "|customer_rating                  |categorical|0         |0.0     |22            |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 57000), ('4.9', 11642), ('4.6', 11533)]                                             |\n",
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------+--------+----------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "\n",
    "# 1. Read raw (string only)\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Clean headers\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# 3. Clean string values (remove triple/double quotes + whitespace)\n",
    "for c in df.columns:\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        F.regexp_replace(F.col(c), '^\"+|\"+$', '')  # remove leading/trailing quotes\n",
    "    ).withColumn(\n",
    "        c,\n",
    "        F.trim(F.col(c))  # strip spaces\n",
    "    )\n",
    "\n",
    "row_count = df.count()\n",
    "\n",
    "# --- Detect numeric vs categorical ---\n",
    "numeric_cols, categorical_cols = [], []\n",
    "for c in df.columns:\n",
    "    tmp = df.withColumn(\"tmp\", F.col(c).cast(\"double\"))\n",
    "    non_nulls = tmp.filter(F.col(c).isNotNull()).count()\n",
    "    cast_success = tmp.filter(F.col(\"tmp\").isNotNull()).count()\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# --- Profiling Report ---\n",
    "report_rows = []\n",
    "\n",
    "for c in df.columns:\n",
    "    null_count = df.filter(F.col(c).isNull() | (F.trim(F.col(c)) == \"\")).count()\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = df.select(c).distinct().count()\n",
    "\n",
    "    # Skew ratio\n",
    "    skew_ratio = round(distinct_count / row_count, 2) if row_count else None\n",
    "    if skew_ratio is None:\n",
    "        skew_level = \"unknown\"\n",
    "    elif skew_ratio < 0.1:\n",
    "        skew_level = \"low\"\n",
    "    elif skew_ratio < 0.5:\n",
    "        skew_level = \"mid\"\n",
    "    else:\n",
    "        skew_level = \"high\"\n",
    "\n",
    "    # Defaults\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outliers = None\n",
    "    top_values = None\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        stats = df.select(\n",
    "            F.min(F.col(c).cast(\"double\")).alias(\"min\"),\n",
    "            F.max(F.col(c).cast(\"double\")).alias(\"max\"),\n",
    "            F.mean(F.col(c).cast(\"double\")).alias(\"mean\"),\n",
    "            F.stddev(F.col(c).cast(\"double\")).alias(\"stddev\")\n",
    "        ).collect()[0]\n",
    "        min_val, max_val, mean_val, stddev_val = stats\n",
    "\n",
    "        # Percentiles / Histogram\n",
    "        percentiles = df.select(F.col(c).cast(\"double\").alias(c)) \\\n",
    "            .na.drop() \\\n",
    "            .approxQuantile(c, [0.25, 0.5, 0.75, 0.95, 0.99], 0.01)\n",
    "\n",
    "        # Outliers = values beyond mean Â± 3*stddev\n",
    "        if mean_val is not None and stddev_val is not None:\n",
    "            outliers = df.filter(\n",
    "                (F.col(c).cast(\"double\") > mean_val + 3 * stddev_val) |\n",
    "                (F.col(c).cast(\"double\") < mean_val - 3 * stddev_val)\n",
    "            ).count()\n",
    "\n",
    "    elif c in categorical_cols:\n",
    "        # Top categorical values\n",
    "        top_vals = df.groupBy(c).count().orderBy(F.desc(\"count\")).limit(3).collect()\n",
    "        top_values = [(row[c], row[\"count\"]) for row in top_vals]\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct, distinct_count,\n",
    "        skew_ratio, skew_level, min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outliers, str(top_values)\n",
    "    ))\n",
    "\n",
    "# 5. Convert to Spark DataFrame\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "     \"percentiles\", \"outliers\", \"top_values\"]\n",
    ")\n",
    "\n",
    "report_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ab3eb-67f5-463e-811a-0a804eb9ddcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
