{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41457af3-580e-4a2f-93bd-db2801e222b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://25980bb9571e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data Quality</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x75be46775b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data Quality\").getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd40ef85-0ad5-42d9-8da0-5bf18007e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"FakeTrainingData\").getOrCreate()\n",
    "\n",
    "# Generate fake data\n",
    "data = [\n",
    "    (44, 9, 21.0),\n",
    "    (10, 5, 53.0),\n",
    "    (37, 6, 80.0),\n",
    "    (19, 10, 11.0),\n",
    "    (46, 13, 215.0)\n",
    "]\n",
    "\n",
    "# Add more rows to reach 543 samples\n",
    "for _ in range(543 - len(data)):\n",
    "    totalAdClicks = random.randint(1, 60)\n",
    "    totalBuyClicks = random.randint(1, 15)\n",
    "    totalRevenue = round(random.uniform(5, 250), 1)\n",
    "    data.append((totalAdClicks, totalBuyClicks, totalRevenue))\n",
    "\n",
    "# Define schema\n",
    "columns = [\"SumAddClicks\", \"SumBuyClicks\", \"Revenue\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fd1538-d214-45a4-82aa-b581b58e79cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------+\n",
      "|SumAddClicks|SumBuyClicks|Revenue|\n",
      "+------------+------------+-------+\n",
      "|          44|           9|   21.0|\n",
      "|          10|           5|   53.0|\n",
      "|          37|           6|   80.0|\n",
      "|          19|          10|   11.0|\n",
      "|          46|          13|  215.0|\n",
      "+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "(543, 3)\n"
     ]
    }
   ],
   "source": [
    "trainingDF = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show first 5 rows\n",
    "trainingDF.show(5)\n",
    "\n",
    "# Show dimensions (rows, columns)\n",
    "rows = trainingDF.count()\n",
    "cols = len(trainingDF.columns)\n",
    "print((rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0c8f1a-58f6-4abe-b138-d6dffe990cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "rows.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68dda5f-9e3f-4edd-846d-653429095c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr, concat, lit\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, LongType\n",
    "import pyspark.sql.functions as F\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "BOOTSTRAP = \"broker:29094\"   # inside Docker network\n",
    "TXN_TOPIC = \"transaction\"\n",
    "METRICS_TOPIC = \"metrics\"\n",
    "mode = \"salting\"   # \"baseline\", \"broadcast\", or \"salting\"\n",
    "enable_aqe = True\n",
    "SKEW_CARD = \"4111-1111-1111-1111\"\n",
    "# ----------------------------\n",
    "# Spark Session\n",
    "# ----------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"fraud-detection-demo\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"6\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark ready:\", spark.version)\n",
    "\n",
    "if enable_aqe:\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# ----------------------------\n",
    "# Input schema & Kafka source\n",
    "# ----------------------------\n",
    "schema = StructType() \\\n",
    "    .add(\"txn_id\", LongType()) \\\n",
    "    .add(\"card_number\", StringType()) \\\n",
    "    .add(\"amount\", DoubleType()) \\\n",
    "    .add(\"merchant\", StringType()) \\\n",
    "    .add(\"ts\", LongType())\n",
    "\n",
    "kdf = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", BOOTSTRAP) \\\n",
    "    .option(\"subscribe\", TXN_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "txn_df = kdf.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"j\")).select(\"j.*\")\n",
    "\n",
    "# ----------------------------\n",
    "# Lookup table (risk profiles)\n",
    "# ----------------------------\n",
    "risk_profiles = spark.createDataFrame([\n",
    "    (\"4111-1111-1111-1111\", \"high\"),\n",
    "    (\"4000-0000-0000-0002\", \"medium\"),\n",
    "    (\"4000-0000-0000-0003\", \"low\")\n",
    "], [\"card_number\", \"risk_level\"])\n",
    "\n",
    "# ----------------------------\n",
    "# Join strategies\n",
    "# ----------------------------\n",
    "if mode == \"baseline\":\n",
    "    joined = txn_df.join(risk_profiles, \"card_number\", \"left\")\n",
    "\n",
    "elif mode == \"broadcast\":\n",
    "    joined = txn_df.join(F.broadcast(risk_profiles), \"card_number\", \"left\")\n",
    "\n",
    "elif mode == \"salting\":\n",
    "    SALT_N = 6\n",
    "    salts = spark.range(0, SALT_N).selectExpr(\"id as salt\")\n",
    "    lookup_salted = risk_profiles.crossJoin(salts) \\\n",
    "        .withColumn(\"salted_card\", concat(col(\"card_number\"), lit(\"_\"), col(\"salt\"))) \\\n",
    "        .select(\"salted_card\", \"risk_level\")\n",
    "\n",
    "    salted_stream = txn_df.withColumn(\n",
    "        \"salt\",\n",
    "        expr(f\"CASE WHEN card_number='{SKEW_CARD}' THEN floor(rand()*{SALT_N}) ELSE 0 END\")\n",
    "    ).withColumn(\"salted_card\", concat(col(\"card_number\"), lit(\"_\"), col(\"salt\")))\n",
    "\n",
    "    joined = salted_stream.join(\n",
    "        lookup_salted,\n",
    "        salted_stream.salted_card == lookup_salted.salted_card,\n",
    "        \"left\"\n",
    "    ).drop(\"salted_card\")\n",
    "\n",
    "# ----------------------------\n",
    "# Metrics sender\n",
    "# ----------------------------\n",
    "def send_metrics(batch_df, batch_id):\n",
    "    total = batch_df.count()\n",
    "    risky = batch_df.filter(\"risk_level='high'\").count()\n",
    "    metrics = {\n",
    "        \"batch_id\": int(batch_id),\n",
    "        \"mode\": mode,\n",
    "        \"total_txns\": int(total),\n",
    "        \"high_risk_txns\": int(risky),\n",
    "        \"fraud_ratio\": round(risky / total, 3) if total > 0 else 0\n",
    "    }\n",
    "    print(f\"[Metrics] {metrics}\")\n",
    "\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=BOOTSTRAP,\n",
    "        value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    "    )\n",
    "    producer.send(METRICS_TOPIC, value=metrics)\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "\n",
    "# ----------------------------\n",
    "# Write stream\n",
    "# ----------------------------\n",
    "query = joined.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(lambda df, bid: (df.show(5, truncate=False), send_metrics(df, bid))) \\\n",
    "    .option(\"checkpointLocation\", f\"/opt/output/fraud_checkpoint_{mode}\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(60)\n",
    "query.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
