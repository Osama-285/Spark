{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3086e740-9107-4ec8-8a82-40a406704521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f5838d8c3611:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CSV Reads Optimization</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7f38796e20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSV Reads Optimization\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd584dd-b162-4a30-b31d-822dc2785f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In Spark, execution is broken into **Jobs, Stages, and Tasks**:\n",
    "\n",
    "# * **Job**:\n",
    "#   A job is triggered by an **action** (like `count()`, `show()`, `collect()`, `write()`).\n",
    "#   Example: calling `df.count()` triggers one job.\n",
    "\n",
    "# * **Stage**:\n",
    "#   Each job is divided into stages, which are separated by **shuffle boundaries**.\n",
    "#   If an operation requires data to be moved across partitions (e.g., `groupBy`, `join`), a new stage is created.\n",
    "#   Example: `df.groupBy(\"city\").count()` creates multiple stages because Spark must shuffle rows by `city`.\n",
    "\n",
    "# * **Task**:\n",
    "#   A task is the **smallest unit of execution**, and there is one task per partition of the data in a stage.\n",
    "#   Tasks are what actually run on executors in parallel.\n",
    "#   Example: if a stage operates on a file split into 8 partitions, Spark creates 8 tasks.\n",
    "\n",
    "\n",
    "# **One job → multiple stages (if shuffles are needed) → multiple tasks (one per partition per stage).**\n",
    "\n",
    "# “If I read a CSV with 7 partitions and run `df.count()`, Spark will create **1 job**, with **1 stage** (because no shuffle is needed), and **7 tasks** (one per partition).\n",
    "\n",
    "# If I do a `groupBy` on that DataFrame, Spark will create **1 job**, split into **2 stages** — one stage to read and map data, and another stage after shuffle to aggregate. Each stage again breaks into tasks, based on the number of partitions.”\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ✅ Key Takeaway (good one-liner for interviews)\n",
    "\n",
    "# * **Job = action**\n",
    "# * **Stage = set of transformations without shuffle**\n",
    "# * **Task = work done on a single partition**\n",
    "\n",
    "# This Action create 7 tasks, 2 jobs and 2 stages\n",
    "\n",
    "df = spark.read.csv(\"/opt/data/ncr_ride_bookings.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea90927-d032-4f94-ad83-586962c3bf97",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unionByName() takes from 2 to 3 positional arguments but 21 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munionByName(\u001b[38;5;241m*\u001b[39mresults[\u001b[38;5;241m1\u001b[39m:])  \u001b[38;5;66;03m# combine results\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Run summary\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m \u001b[43mskew_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m summary_df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mskew_summary\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      7\u001b[0m     stats \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      8\u001b[0m         df\u001b[38;5;241m.\u001b[39mgroupBy(col)\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m      9\u001b[0m           \u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskew_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_count\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_count\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(stats)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munionByName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unionByName() takes from 2 to 3 positional arguments but 21 were given"
     ]
    }
   ],
   "source": [
    "# df.show()\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def skew_summary(df):\n",
    "    results = []\n",
    "    for col in df.columns:\n",
    "        stats = (\n",
    "            df.groupBy(col).count()\n",
    "              .agg(\n",
    "                  F.count(\"*\").alias(\"distinct_values\"),\n",
    "                  F.min(\"count\").alias(\"min_count\"),\n",
    "                  F.expr(\"percentile_approx(count, 0.5)\").alias(\"median_count\"),\n",
    "                  F.mean(\"count\").alias(\"mean_count\"),\n",
    "                  F.max(\"count\").alias(\"max_count\")\n",
    "              )\n",
    "              .withColumn(\"column\", F.lit(col))\n",
    "              .withColumn(\"skew_ratio\", F.col(\"max_count\") / F.col(\"mean_count\"))\n",
    "              .withColumn(\n",
    "                  \"skew_class\",\n",
    "                  F.when(F.col(\"skew_ratio\") <= 2, \"Balanced\")\n",
    "                   .when((F.col(\"skew_ratio\") > 2) & (F.col(\"skew_ratio\") <= 10), \"Moderately Skewed\")\n",
    "                   .otherwise(\"Highly Skewed\")\n",
    "              )\n",
    "        )\n",
    "        results.append(stats)\n",
    "    \n",
    "    # Merge all per-column results into one DataFrame\n",
    "    summary_df = reduce(lambda a, b: a.unionByName(b), results)\n",
    "    return summary_df\n",
    "\n",
    "# Run summary\n",
    "summary_df = skew_summary(df)\n",
    "summary_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b49328-7276-43cc-9fb0-5b03558182a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Booking ID: string (nullable = true)\n",
      " |-- Booking Status: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Vehicle Type: string (nullable = true)\n",
      " |-- Pickup Location: string (nullable = true)\n",
      " |-- Drop Location: string (nullable = true)\n",
      " |-- Avg VTAT: string (nullable = true)\n",
      " |-- Avg CTAT: string (nullable = true)\n",
      " |-- Cancelled Rides by Customer: string (nullable = true)\n",
      " |-- Reason for cancelling by Customer: string (nullable = true)\n",
      " |-- Cancelled Rides by Driver: string (nullable = true)\n",
      " |-- Driver Cancellation Reason: string (nullable = true)\n",
      " |-- Incomplete Rides: string (nullable = true)\n",
      " |-- Incomplete Rides Reason: string (nullable = true)\n",
      " |-- Booking Value: string (nullable = true)\n",
      " |-- Ride Distance: string (nullable = true)\n",
      " |-- Driver Ratings: string (nullable = true)\n",
      " |-- Customer Rating: string (nullable = true)\n",
      " |-- Payment Method: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db2582b-ffaa-42bd-a645-65b1ee12c1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check DF Type\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Assume df is your object\n",
    "isinstance(df, DataFrame) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "294e1e76-83d6-41d6-ae96-e88572494781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+--------------+-----------+------------+---------------+-------------+--------+--------+---------------------------+---------------------------------+-------------------------+--------------------------+----------------+-----------------------+-------------+-------------+--------------+---------------+--------------+\n",
      "|Date|Time|Booking ID|Booking Status|Customer ID|Vehicle Type|Pickup Location|Drop Location|Avg VTAT|Avg CTAT|Cancelled Rides by Customer|Reason for cancelling by Customer|Cancelled Rides by Driver|Driver Cancellation Reason|Incomplete Rides|Incomplete Rides Reason|Booking Value|Ride Distance|Driver Ratings|Customer Rating|Payment Method|\n",
      "+----+----+----------+--------------+-----------+------------+---------------+-------------+--------+--------+---------------------------+---------------------------------+-------------------------+--------------------------+----------------+-----------------------+-------------+-------------+--------------+---------------+--------------+\n",
      "| 0.0| 0.0|       0.0|           0.0|        0.0|         0.0|            0.0|          0.0|     0.0|     0.0|                        0.0|                              0.0|                      0.0|                       0.0|             0.0|                    0.0|          0.0|          0.0|           0.0|            0.0|           0.0|\n",
      "+----+----+----------+--------------+-----------+------------+---------------+-------------+--------+--------+---------------------------+---------------------------------+-------------------------+--------------------------+----------------+-----------------------+-------------+-------------+--------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "total_count = df.count()\n",
    "null_stats = df.select([\n",
    "    (count(when(col(c).isNull(), c)) / total_count).alias(c) for c in df.columns\n",
    "])\n",
    "\n",
    "null_stats.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d885175-5e4e-447b-ba95-80b261014722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2024, 3, 23), Time=datetime.datetime(2025, 9, 13, 12, 29, 38), Booking ID='\"\"\"CNR5884300\"\"\"', Booking Status='No Driver Found', Customer ID='\"\"\"CID1982111\"\"\"', Vehicle Type='eBike', Pickup Location='Palam Vihar', Drop Location='Jhilmil', Avg VTAT='null', Avg CTAT='null', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='null', Ride Distance='null', Driver Ratings='null', Customer Rating='null', Payment Method='null'),\n",
       " Row(Date=datetime.date(2024, 11, 29), Time=datetime.datetime(2025, 9, 13, 18, 1, 39), Booking ID='\"\"\"CNR1326809\"\"\"', Booking Status='Incomplete', Customer ID='\"\"\"CID4604802\"\"\"', Vehicle Type='Go Sedan', Pickup Location='Shastri Nagar', Drop Location='Gurgaon Sector 56', Avg VTAT='4.9', Avg CTAT='14.0', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='1', Incomplete Rides Reason='Vehicle Breakdown', Booking Value='237', Ride Distance='5.73', Driver Ratings='null', Customer Rating='null', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 8, 23), Time=datetime.datetime(2025, 9, 13, 8, 56, 10), Booking ID='\"\"\"CNR8494506\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID9202816\"\"\"', Vehicle Type='Auto', Pickup Location='Khandsa', Drop Location='Malviya Nagar', Avg VTAT='13.4', Avg CTAT='25.8', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='627', Ride Distance='13.58', Driver Ratings='4.9', Customer Rating='4.9', Payment Method='Debit Card'),\n",
       " Row(Date=datetime.date(2024, 10, 21), Time=datetime.datetime(2025, 9, 13, 17, 17, 25), Booking ID='\"\"\"CNR8906825\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID2610914\"\"\"', Vehicle Type='Premier Sedan', Pickup Location='Central Secretariat', Drop Location='Inderlok', Avg VTAT='13.1', Avg CTAT='28.5', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='416', Ride Distance='34.02', Driver Ratings='4.6', Customer Rating='5.0', Payment Method='UPI'),\n",
       " Row(Date=datetime.date(2024, 9, 16), Time=datetime.datetime(2025, 9, 13, 22, 8), Booking ID='\"\"\"CNR1950162\"\"\"', Booking Status='Completed', Customer ID='\"\"\"CID9933542\"\"\"', Vehicle Type='Bike', Pickup Location='Ghitorni Village', Drop Location='Khan Market', Avg VTAT='5.3', Avg CTAT='19.6', Cancelled Rides by Customer='null', Reason for cancelling by Customer='null', Cancelled Rides by Driver='null', Driver Cancellation Reason='null', Incomplete Rides='null', Incomplete Rides Reason='null', Booking Value='737', Ride Distance='48.21', Driver Ratings='4.1', Customer Rating='4.3', Payment Method='UPI')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6340578e-d99f-4a8a-a813-82446ba272de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf3353-95e4-45f7-aa8b-ddea836ffd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"Date\", \"Vehicle Type\").parquet(\"/opt/output/partitioned_rides\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878015d9-980f-4593-b27f-57ec316eaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.bucketBy(16, \"Customer ID\").sortBy(\"Customer ID\").saveAsTable(\"bucketed_rides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5273e86-e0c0-4370-bd8f-90975f104a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
