{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760fe5c4-0d85-4343-8592-58ae25ffbf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f5838d8c3611:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataProfilingAndQualityPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f95d0748820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"DataProfilingAndQualityPipeline\")\n",
    "        # Executor/driver configs\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")  \n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.orc.impl\", \"native\")\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"600\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee30b18-5820-4735-8fc3-0fd1a40579db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "output_path = \"/data/processed/output.parquet\"\n",
    "\n",
    "def cleanColumnName(col_name):\n",
    "    col_name = col_name.strip()\n",
    "\n",
    "    col_name = re.sub(r\"[.\\s\\-]+\", \"_\", col_name)\n",
    "    col_name = re.sub(r\"[^0-9a-zA-Z_]\", \"\", col_name)\n",
    "    col_name = col_name.lower()\n",
    "    col_name = re.sub(r\"^_+|_+$\", \"\", col_name)\n",
    "    col_name = re.sub(r\"_+\", \"_\", col_name)\n",
    "    \n",
    "    return col_name\n",
    "\n",
    "header = spark.sparkContext.textFile(input_path).first().split(\",\")\n",
    "cleaned_headers = [cleanColumnName(h) for h in header]\n",
    "\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=True).toDF(*cleaned_headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade11cbc-0481-4ee3-9aeb-e7d8d2efdd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------+--------+----------------------------------------------------------------------------------------------+\n",
      "|column_name                      |data_type  |null_count|null_pct|distinct_count|skew_ratio|skew_level|min_val|max_val|mean_val         |stddev_val        |percentiles                 |outliers|top_values                                                                                    |\n",
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------+--------+----------------------------------------------------------------------------------------------+\n",
      "|date                             |categorical|0         |0.0     |365           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('2024-11-16', 462), ('2024-09-18', 456), ('2024-05-09', 456)]                               |\n",
      "|time                             |categorical|0         |0.0     |62910         |0.42      |mid       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('17:44:57', 16), ('19:17:33', 12), ('18:59:55', 11)]                                        |\n",
      "|booking_id                       |categorical|0         |0.0     |148767        |0.99      |high      |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('CNR3648267', 3), ('CNR9603232', 3), ('CNR7908610', 3)]                                     |\n",
      "|booking_status                   |categorical|0         |0.0     |5             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Completed', 93000), ('Cancelled by Driver', 27000), ('No Driver Found', 10500)]            |\n",
      "|customer_id                      |categorical|0         |0.0     |148788        |0.99      |high      |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('CID7828101', 3), ('CID6715450', 3), ('CID6468528', 3)]                                     |\n",
      "|vehicle_type                     |categorical|0         |0.0     |7             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Auto', 37419), ('Go Mini', 29806), ('Go Sedan', 27141)]                                    |\n",
      "|pickup_location                  |categorical|0         |0.0     |176           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Khandsa', 949), ('Barakhamba Road', 946), ('Saket', 931)]                                  |\n",
      "|drop_location                    |categorical|0         |0.0     |176           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('Ashram', 936), ('Basai Dhankot', 917), ('Lok Kalyan Marg', 916)]                           |\n",
      "|avg_vtat                         |numeric    |0         |0.0     |182           |0.0       |low       |2.0    |20.0   |8.456351971326171|3.7735638264095708|[5.3, 8.2, 11.2, 14.4, 20.0]|200     |None                                                                                          |\n",
      "|avg_ctat                         |categorical|0         |0.0     |352           |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 48000), ('24.8', 401), ('25.9', 389)]                                               |\n",
      "|cancelled_rides_by_customer      |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 139500), ('1', 10500)]                                                              |\n",
      "|reason_for_cancelling_by_customer|categorical|0         |0.0     |6             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 139500), ('Wrong Address', 2362), ('Change of plans', 2353)]                        |\n",
      "|cancelled_rides_by_driver        |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 123000), ('1', 27000)]                                                              |\n",
      "|driver_cancellation_reason       |categorical|0         |0.0     |5             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 123000), ('Customer related issue', 6837), ('The customer was coughing/sick', 6751)]|\n",
      "|incomplete_rides                 |categorical|0         |0.0     |2             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 141000), ('1', 9000)]                                                               |\n",
      "|incomplete_rides_reason          |categorical|0         |0.0     |4             |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 141000), ('Customer Demand', 3040), ('Vehicle Breakdown', 3012)]                    |\n",
      "|booking_value                    |categorical|0         |0.0     |2567          |0.02      |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 48000), ('176', 177), ('125', 174)]                                                 |\n",
      "|ride_distance                    |categorical|0         |0.0     |4902          |0.03      |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 48000), ('17.31', 43), ('9.61', 43)]                                                |\n",
      "|driver_ratings                   |categorical|0         |0.0     |22            |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 57000), ('4.3', 14081), ('4.2', 13841)]                                             |\n",
      "|customer_rating                  |categorical|0         |0.0     |22            |0.0       |low       |NULL   |NULL   |NULL             |NULL              |None                        |NULL    |[('null', 57000), ('4.9', 11642), ('4.6', 11533)]                                             |\n",
      "+---------------------------------+-----------+----------+--------+--------------+----------+----------+-------+-------+-----------------+------------------+----------------------------+--------+----------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "input_path = \"/opt/data/ncr_ride_bookings.csv\"\n",
    "\n",
    "# 1. Read raw (string only)\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Clean headers\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# 3. Clean string values (remove triple/double quotes + whitespace)\n",
    "for c in df.columns:\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        F.regexp_replace(F.col(c), '^\"+|\"+$', '')  # remove leading/trailing quotes\n",
    "    ).withColumn(\n",
    "        c,\n",
    "        F.trim(F.col(c))  # strip spaces\n",
    "    )\n",
    "\n",
    "row_count = df.count()\n",
    "\n",
    "# --- Detect numeric vs categorical ---\n",
    "numeric_cols, categorical_cols = [], []\n",
    "for c in df.columns:\n",
    "    tmp = df.withColumn(\"tmp\", F.col(c).cast(\"double\"))\n",
    "    non_nulls = tmp.filter(F.col(c).isNotNull()).count()\n",
    "    cast_success = tmp.filter(F.col(\"tmp\").isNotNull()).count()\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# --- Profiling Report ---\n",
    "report_rows = []\n",
    "\n",
    "for c in df.columns:\n",
    "    null_count = df.filter(F.col(c).isNull() | (F.trim(F.col(c)) == \"\")).count()\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = df.select(c).distinct().count()\n",
    "\n",
    "    # Skew ratio\n",
    "    skew_ratio = round(distinct_count / row_count, 2) if row_count else None\n",
    "    if skew_ratio is None:\n",
    "        skew_level = \"unknown\"\n",
    "    elif skew_ratio < 0.1:\n",
    "        skew_level = \"low\"\n",
    "    elif skew_ratio < 0.5:\n",
    "        skew_level = \"mid\"\n",
    "    else:\n",
    "        skew_level = \"high\"\n",
    "\n",
    "    # Defaults\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outliers = None\n",
    "    top_values = None\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        stats = df.select(\n",
    "            F.min(F.col(c).cast(\"double\")).alias(\"min\"),\n",
    "            F.max(F.col(c).cast(\"double\")).alias(\"max\"),\n",
    "            F.mean(F.col(c).cast(\"double\")).alias(\"mean\"),\n",
    "            F.stddev(F.col(c).cast(\"double\")).alias(\"stddev\")\n",
    "        ).collect()[0]\n",
    "        min_val, max_val, mean_val, stddev_val = stats\n",
    "\n",
    "        # Percentiles / Histogram\n",
    "        percentiles = df.select(F.col(c).cast(\"double\").alias(c)) \\\n",
    "            .na.drop() \\\n",
    "            .approxQuantile(c, [0.25, 0.5, 0.75, 0.95, 0.99], 0.01)\n",
    "\n",
    "        # Outliers = values beyond mean ± 3*stddev\n",
    "        if mean_val is not None and stddev_val is not None:\n",
    "            outliers = df.filter(\n",
    "                (F.col(c).cast(\"double\") > mean_val + 3 * stddev_val) |\n",
    "                (F.col(c).cast(\"double\") < mean_val - 3 * stddev_val)\n",
    "            ).count()\n",
    "\n",
    "    elif c in categorical_cols:\n",
    "        # Top categorical values\n",
    "        top_vals = df.groupBy(c).count().orderBy(F.desc(\"count\")).limit(3).collect()\n",
    "        top_values = [(row[c], row[\"count\"]) for row in top_vals]\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct, distinct_count,\n",
    "        skew_ratio, skew_level, min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outliers, str(top_values)\n",
    "    ))\n",
    "\n",
    "# 5. Convert to Spark DataFrame\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "     \"percentiles\", \"outliers\", \"top_values\"]\n",
    ")\n",
    "\n",
    "report_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7ab3eb-67f5-463e-811a-0a804eb9ddcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Syntax error in attribute name: pk\u0003\u0004\u0014\u0000\u0006\u0000\b\u0000\u0000\u0000!\u0000�j�gl\u0001\u0000\u0000\u0010\u0005\u0000\u0000\u0013\u0000\b\u0002[content_types]_xml_�\u0004\u0002(�\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000��mn�0\u0010��h�!�\u0016%ny_��vq`\t�(\u00070����ؖ�-�홸?b���f\u0013+��}/o\u001c�&��dk\b��-ٰ\u0018�\f�tj�e�����=�0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. Clean string values (remove extra quotes and whitespace)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     17\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     18\u001b[0m         c,\n\u001b[0;32m---> 19\u001b[0m         F\u001b[38;5;241m.\u001b[39mregexp_replace(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# remove leading/trailing quotes\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     )\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     21\u001b[0m         c,\n\u001b[1;32m     22\u001b[0m         F\u001b[38;5;241m.\u001b[39mtrim(F\u001b[38;5;241m.\u001b[39mcol(c))  \u001b[38;5;66;03m# strip spaces\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Cache so we don’t re-read multiple times\u001b[39;00m\n\u001b[1;32m     26\u001b[0m df\u001b[38;5;241m.\u001b[39mcache()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/functions.py:223\u001b[0m, in \u001b[0;36mcol\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@try_remote_functions\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcol\u001b[39m(col: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    Returns a :class:`~pyspark.sql.Column` based on the given column name.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    Column<'x'>\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/functions.py:97\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[43mjf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Syntax error in attribute name: pk\u0003\u0004\u0014\u0000\u0006\u0000\b\u0000\u0000\u0000!\u0000�j�gl\u0001\u0000\u0000\u0010\u0005\u0000\u0000\u0013\u0000\b\u0002[content_types]_xml_�\u0004\u0002(�\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000��mn�0\u0010��h�!�\u0016%ny_��vq`\t�(\u00070����ؖ�-�홸?b���f\u0013+��}/o\u001c�&��dk\b��-ٰ\u0018�\f�tj�e�����=�0."
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql import types as T\n",
    "\n",
    "input_path = \"/opt/data/ac_acct.csv\"\n",
    "\n",
    "# 1. Read raw (string only)\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Clean headers\n",
    "def clean_col(colname: str) -> str:\n",
    "    return colname.strip().lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "df = df.toDF(*[clean_col(c) for c in df.columns])\n",
    "\n",
    "# 3. Clean string values (remove extra quotes and whitespace)\n",
    "for c in df.columns:\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        F.regexp_replace(F.col(c), '^\"+|\"+$', '')  # remove leading/trailing quotes\n",
    "    ).withColumn(\n",
    "        c,\n",
    "        F.trim(F.col(c))  # strip spaces\n",
    "    )\n",
    "\n",
    "# Cache so we don’t re-read multiple times\n",
    "df.cache()\n",
    "row_count = df.count()\n",
    "\n",
    "# --- Detect numeric vs categorical ---\n",
    "numeric_cols, categorical_cols = [], []\n",
    "for c in df.columns:\n",
    "    tmp = df.withColumn(\"tmp\", F.col(c).cast(\"double\"))\n",
    "    non_nulls = tmp.filter(F.col(c).isNotNull()).count()\n",
    "    cast_success = tmp.filter(F.col(\"tmp\").isNotNull()).count()\n",
    "    if non_nulls > 0 and (cast_success / non_nulls) > 0.9:\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# --- 1. Batch distinct + null counts ---\n",
    "agg_exprs = []\n",
    "for c in df.columns:\n",
    "    agg_exprs.append(\n",
    "        F.sum(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), 1).otherwise(0)).alias(f\"{c}_nulls\")\n",
    "    )\n",
    "    agg_exprs.append(F.countDistinct(c).alias(f\"{c}_distinct\"))\n",
    "\n",
    "nulls_distinct_df = df.agg(*agg_exprs)\n",
    "\n",
    "# --- 2. Batch numeric stats ---\n",
    "agg_exprs = []\n",
    "for c in numeric_cols:\n",
    "    agg_exprs += [\n",
    "        F.min(F.col(c).cast(\"double\")).alias(f\"{c}_min\"),\n",
    "        F.max(F.col(c).cast(\"double\")).alias(f\"{c}_max\"),\n",
    "        F.mean(F.col(c).cast(\"double\")).alias(f\"{c}_mean\"),\n",
    "        F.stddev(F.col(c).cast(\"double\")).alias(f\"{c}_stddev\"),\n",
    "    ]\n",
    "\n",
    "numeric_stats_df = df.agg(*agg_exprs)\n",
    "\n",
    "# Collect results into dicts for easy lookup\n",
    "nulls_distinct = nulls_distinct_df.collect()[0].asDict()\n",
    "numeric_stats = numeric_stats_df.collect()[0].asDict()\n",
    "\n",
    "# --- 3. Percentiles (one pass per numeric col) ---\n",
    "percentiles_dict = {}\n",
    "for c in numeric_cols:\n",
    "    percentiles = df.select(F.col(c).cast(\"double\").alias(c)) \\\n",
    "        .na.drop() \\\n",
    "        .approxQuantile(c, [0.25, 0.5, 0.75, 0.95, 0.99], 0.01)\n",
    "    percentiles_dict[c] = percentiles\n",
    "\n",
    "# --- 4. Top values for categorical cols ---\n",
    "top_values_dict = {}\n",
    "for c in categorical_cols:\n",
    "    top_vals = df.groupBy(c).count().orderBy(F.desc(\"count\")).limit(3).collect()\n",
    "    top_values_dict[c] = [(row[c], row[\"count\"]) for row in top_vals]\n",
    "\n",
    "# --- Build final report ---\n",
    "report_rows = []\n",
    "for c in df.columns:\n",
    "    null_count = nulls_distinct[f\"{c}_nulls\"]\n",
    "    null_pct = round((null_count / row_count) * 100, 2) if row_count else None\n",
    "    distinct_count = nulls_distinct[f\"{c}_distinct\"]\n",
    "\n",
    "    skew_ratio = round(distinct_count / row_count, 2) if row_count else None\n",
    "    if skew_ratio is None:\n",
    "        skew_level = \"unknown\"\n",
    "    elif skew_ratio < 0.1:\n",
    "        skew_level = \"low\"\n",
    "    elif skew_ratio < 0.5:\n",
    "        skew_level = \"mid\"\n",
    "    else:\n",
    "        skew_level = \"high\"\n",
    "\n",
    "    min_val = max_val = mean_val = stddev_val = None\n",
    "    percentiles = None\n",
    "    outliers = None\n",
    "    top_values = None\n",
    "    dtype = \"numeric\" if c in numeric_cols else \"categorical\"\n",
    "\n",
    "    if c in numeric_cols:\n",
    "        min_val = numeric_stats.get(f\"{c}_min\")\n",
    "        max_val = numeric_stats.get(f\"{c}_max\")\n",
    "        mean_val = numeric_stats.get(f\"{c}_mean\")\n",
    "        stddev_val = numeric_stats.get(f\"{c}_stddev\")\n",
    "        percentiles = percentiles_dict.get(c)\n",
    "\n",
    "        if mean_val is not None and stddev_val is not None:\n",
    "            outliers = df.filter(\n",
    "                (F.col(c).cast(\"double\") > mean_val + 3 * stddev_val) |\n",
    "                (F.col(c).cast(\"double\") < mean_val - 3 * stddev_val)\n",
    "            ).count()\n",
    "\n",
    "    elif c in categorical_cols:\n",
    "        top_values = top_values_dict.get(c)\n",
    "\n",
    "    report_rows.append((\n",
    "        c, dtype, null_count, null_pct, distinct_count,\n",
    "        skew_ratio, skew_level, min_val, max_val, mean_val, stddev_val,\n",
    "        str(percentiles), outliers, str(top_values)\n",
    "    ))\n",
    "\n",
    "# --- Convert to Spark DataFrame ---\n",
    "report_df = spark.createDataFrame(\n",
    "    report_rows,\n",
    "    [\"column_name\", \"data_type\", \"null_count\", \"null_pct\",\n",
    "     \"distinct_count\", \"skew_ratio\", \"skew_level\",\n",
    "     \"min_val\", \"max_val\", \"mean_val\", \"stddev_val\",\n",
    "     \"percentiles\", \"outliers\", \"top_values\"]\n",
    ")\n",
    "\n",
    "report_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058820f-be2e-41ee-a368-dbd12bed106b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
